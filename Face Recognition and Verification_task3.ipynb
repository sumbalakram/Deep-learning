{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ugHy94sLV6sX"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch, os\n",
    "from random import shuffle\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "L8j8xCxNI6Ld"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "label_encoder = preprocessing.LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "6WvoQYKw2bO7",
    "outputId": "56caec73-4596-40ef-deb4-e568e51bc96d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>identity</th>\n",
       "      <th>Male</th>\n",
       "      <th>Eyeglasses</th>\n",
       "      <th>Smiling</th>\n",
       "      <th>Wearing_Hat</th>\n",
       "      <th>Young</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000001.jpg</td>\n",
       "      <td>2880</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000002.jpg</td>\n",
       "      <td>2937</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000003.jpg</td>\n",
       "      <td>8692</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000004.jpg</td>\n",
       "      <td>5805</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000005.jpg</td>\n",
       "      <td>9295</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     image_id  identity  Male  Eyeglasses  Smiling  Wearing_Hat  Young\n",
       "0  000001.jpg      2880     0           0        1            0      1\n",
       "1  000002.jpg      2937     0           0        1            0      1\n",
       "2  000003.jpg      8692     1           0        0            0      1\n",
       "3  000004.jpg      5805     0           0        0            0      1\n",
       "4  000005.jpg      9295     0           0        0            0      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_attr = pd.read_csv('/MSDS/3rd Sem/ML/Assignment/CelebA/CelebA/Anno/list_attr_celeba.txt',skiprows=1,delim_whitespace=True)\n",
    "df_attr['image_id'] = df_attr.index\n",
    "df_attr.reset_index(inplace=True,level=0)\n",
    "df_attr.drop('index', axis=1, inplace=True)\n",
    "\n",
    "df_identity = pd.read_csv('/MSDS/3rd Sem/ML/Assignment/CelebA/CelebA/Anno/identity_CelebA.txt',skiprows=0,delim_whitespace=True,names=['image_id','identity'])\n",
    "\n",
    "sample_df = pd.merge(df_attr,df_identity)\n",
    "\n",
    "identity_labels = df_identity['identity'].values.tolist()\n",
    "random_list = identity_labels.copy()\n",
    "random_list = list(set(random_list))\n",
    "\n",
    "sample_100_labels = random_list[0:3]\n",
    "sample_df_identity = df_identity[df_identity['identity'].isin(sample_100_labels)]\n",
    "\n",
    "att_df = sample_df[['image_id','identity','Male','Eyeglasses','Smiling','Wearing_Hat','Young']]\n",
    "att_df=att_df[['image_id','identity','Male','Eyeglasses','Smiling','Wearing_Hat','Young']].replace(-1,0)\n",
    "att_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "mh0yDDvf6EJg"
   },
   "outputs": [],
   "source": [
    "sample_images_names = att_df['image_id'].values.tolist()\n",
    "att_df['identity'] = label_encoder.fit_transform(att_df['identity'])\n",
    "sample_images_labels = att_df['identity'].values.tolist()\n",
    "sample_images_features = att_df[['Male','Eyeglasses','Smiling','Wearing_Hat','Young']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "SCEu-YAZ71xN"
   },
   "outputs": [],
   "source": [
    "ngpu = 1\n",
    "device = torch.device('cuda:0' if (\n",
    "    torch.cuda.is_available() and ngpu > 0) else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "6oZ_PeSl7qhC"
   },
   "outputs": [],
   "source": [
    "## Create a custom Dataset class\n",
    "class CelebADataset(Dataset):\n",
    "  def __init__(self, root_dir, transform=None):\n",
    "    \n",
    "    # Read names of images in the root directory\n",
    "    # image_names = os.listdir(root_dir)\n",
    "    image_names = sample_images_names\n",
    "\n",
    "    self.root_dir = root_dir\n",
    "    self.transform = transform \n",
    "    self.image_names = image_names\n",
    "    self.labels = sample_images_features\n",
    "\n",
    "  def __len__(self): \n",
    "    return len(self.image_names)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # Get the path to the image \n",
    "    img_path = os.path.join(self.root_dir, self.image_names[idx])\n",
    "\n",
    "    # Load image and convert it to RGB\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "    # Apply transformations to the image\n",
    "    if self.transform:\n",
    "      img = self.transform(img)\n",
    "    labels = self.labels[idx]\n",
    "    male = torch.tensor(int(labels[0]), dtype=torch.int64)\n",
    "    eye_glasses = torch.tensor(int(labels[1]), dtype=torch.int64)\n",
    "    smiling = torch.tensor(int(labels[2]), dtype=torch.int64)\n",
    "    wearing_hat = torch.tensor(int(labels[3]), dtype=torch.int64)\n",
    "    young = torch.tensor(int(labels[4]), dtype=torch.int64)\n",
    "\n",
    "    return {'img':img,'male':male, 'eye_glasses':eye_glasses, \n",
    "            'smiling':smiling, 'wearing_hat':wearing_hat,\n",
    "            'young':young\n",
    "            }\n",
    "\n",
    "def train_val_dataset(dataset, test_ratio=0.20):\n",
    "    train_idx, test_idx = train_test_split(list(range(len(dataset))), test_size=test_ratio)\n",
    "    datasets = {}\n",
    "    datasets['train'] = Subset(dataset, train_idx)\n",
    "    datasets['test'] = Subset(dataset, test_idx)\n",
    "    return datasets\n",
    "\n",
    "## Load the dataset \n",
    "# Path to directory with all the images\n",
    "img_folder ='/MSDS/3rd Sem/ML/Assignment/CelebA/CelebA/Img/img_align_celeba'\n",
    "\n",
    "# Transformations to be applied to each individual image sample\n",
    "transform=transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "# Load the dataset from file and apply transformations\n",
    "celeba_dataset = CelebADataset(img_folder, transform)\n",
    "datasets = train_val_dataset(celeba_dataset)\n",
    " \n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "# # Number of workers for the dataloader\n",
    "num_workers = 0 if device.type == 'cuda' else 2\n",
    "# # Whether to put fetched data tensors to pinned memory\n",
    "pin_memory = True if device.type == 'cuda' else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8P-CXTH9uwk",
    "outputId": "023e0449-6bda-4853-9b55-61d014c09c61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x28863a55c70>,\n",
       " 'test': <torch.utils.data.dataloader.DataLoader at 0x28863a55370>}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = datasets['train']\n",
    "val_set = datasets['test']\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(datasets['train'],\n",
    "                                                batch_size=batch_size,\n",
    "                                                num_workers=num_workers,\n",
    "                                                pin_memory=pin_memory,\n",
    "                                                shuffle=True)\n",
    "val_dataloader = torch.utils.data.DataLoader(datasets['test'],\n",
    "                                                batch_size=batch_size,\n",
    "                                                num_workers=num_workers,\n",
    "                                                pin_memory=pin_memory,\n",
    "                                                shuffle=False)\n",
    "\n",
    "loaders = {\n",
    "    'train' : train_dataloader,\n",
    "    \n",
    "    'test'  : val_dataloader ,\n",
    "}\n",
    "loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65tSvJsGGZp9",
    "outputId": "4f289612-87b7-4940-f38b-2e354997fd1d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiTaskLearning(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc1): Linear(in_features=32768, out_features=1000, bias=True)\n",
       "  (fc2): Linear(in_features=1000, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiTaskLearning(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiTaskLearning, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=3, # gray-scale images           \n",
    "                # out_channels=256,\n",
    "                out_channels=64,            \n",
    "                kernel_size=5, # 5x5 convolutional kernel               \n",
    "                stride=1,  #no. of pixels pass at a time                 \n",
    "                padding=2, # to preserve size of input image                 \n",
    "            ),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),    \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(64, 32, 5, 1, 2),    \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        )\n",
    "        self.conv3 = nn.Sequential(         \n",
    "            nn.Conv2d(32, 16, 3, 1, 2),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        )\n",
    "        # fully connected layers\n",
    "        self.fc1 = nn.Linear(32*32*32, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 2)\n",
    "\n",
    "\n",
    "    def forward(self, x1):\n",
    "        x1 = self.conv1(x1)\n",
    "        x1 = self.conv2(x1)\n",
    "        x1 = x1.view(x1.size(0), -1)    \n",
    "        x1 = self.fc1(x1)\n",
    "\n",
    "        identity =  x1 \n",
    "        gender = self.fc2(x1)\n",
    "        eye_glasses = self.fc2(x1)\n",
    "        smiling = self.fc2(x1)\n",
    "        wearing_hat = self.fc2(x1)\n",
    "        young = self.fc2(x1)   \n",
    "\n",
    "        return gender,eye_glasses,smiling,wearing_hat,young\n",
    "\n",
    "multi=MultiTaskLearning()\n",
    "multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "IgxS4OHPAbDn"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(multi.parameters(), lr = 0.001)   \n",
    "loss_func = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "pjvCc5zZF9cS"
   },
   "outputs": [],
   "source": [
    "def train(num_epochs, multi, loaders):\n",
    "    multi.to(device).train()\n",
    "   \n",
    "    # Train the model\n",
    "    total_step = len(loaders['train'])\n",
    "    loss_total=[]\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, data in enumerate(loaders['train']):\n",
    "            # gives batch data, normalize x when iterate train_loader\n",
    "            images = data['img'].to(device)\n",
    "            male = data['male'].to(device)\n",
    "            eye_glasses = data['eye_glasses'].to(device)\n",
    "            smiling = data['smiling'].to(device)\n",
    "            wearing_hat = data['wearing_hat'].to(device)\n",
    "            young = data['young'].to(device)\n",
    "\n",
    "            b_x = Variable(images) # batch x\n",
    "            male_y = Variable(male)\n",
    "            eye_glasses_y = Variable(eye_glasses)\n",
    "            smiling_y = Variable(smiling)\n",
    "            wearing_hat_y = Variable(wearing_hat)\n",
    "            young_y = Variable(young)\n",
    "            \n",
    "            male_y = male_y.unsqueeze(1)\n",
    "            eye_glasses_y = eye_glasses_y.unsqueeze(1)\n",
    "            smiling_y = smiling_y.unsqueeze(1)\n",
    "            wearing_hat_y = wearing_hat_y.unsqueeze(1)\n",
    "            young_y = young_y.unsqueeze(1)\n",
    "\n",
    "            output = multi(b_x)\n",
    "            # print(output)\n",
    "            (prd_male, prd_eye_glasses, prd_smiling, prd_wearing_hat, prd_young) = output\n",
    "            # print(prd_male, prd_eye_glasses, prd_smiling, prd_wearing_hat, prd_young)\n",
    "            prd_male = torch.max(prd_male, 1)[1]\n",
    "            prd_eye_glasses = torch.max(prd_eye_glasses, 1)[1]\n",
    "            prd_smiling = torch.max(prd_smiling, 1)[1]\n",
    "            prd_wearing_hat = torch.max(prd_wearing_hat, 1)[1]\n",
    "            prd_young = torch.max(prd_young, 1)[1]\n",
    "           \n",
    "            # Need to threshold them\n",
    "            \n",
    "            male_y = torch.flatten(male_y, start_dim=0, end_dim=1)\n",
    "            eye_glasses_y = torch.flatten(eye_glasses_y, start_dim=0, end_dim=1)\n",
    "            smiling_y = torch.flatten(smiling_y, start_dim=0, end_dim=1)\n",
    "            wearing_hat_y = torch.flatten(wearing_hat_y, start_dim=0, end_dim=1)\n",
    "            young_y = torch.flatten(young_y, start_dim=0, end_dim=1)\n",
    "           \n",
    "            loss_male = loss_func(male_y.float(),prd_male.float())\n",
    "            loss_eye_glasses = loss_func(eye_glasses_y.float(),prd_eye_glasses.float())\n",
    "            loss_smiling = loss_func(smiling_y.float(),prd_smiling.float())\n",
    "            loss_wearing_hat = loss_func(wearing_hat_y.float(),prd_wearing_hat.float())\n",
    "            loss_young = loss_func(young_y.float(),prd_young.float())\n",
    "\n",
    "\n",
    "            t_loss = loss_male+loss_eye_glasses+loss_smiling+loss_wearing_hat+loss_young\n",
    "                  \n",
    "            optimizer.step()                \n",
    "            loss_total.append(t_loss.item())\n",
    "            if (i+1) % 100 == 1:\n",
    "              print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, t_loss.item()))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "YrsSO0v6Aftx",
    "outputId": "c32282df-d5f8-4ca4-a70f-ed6b21f04ee9"
   },
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "train(num_epochs, multi, loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a4wH_6b1hU0Q",
    "outputId": "2384a9c9-563a-4a11-9de0-dc119543c9c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiTaskLearning(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc1): Linear(in_features=32768, out_features=1000, bias=True)\n",
       "  (fc2): Linear(in_features=1000, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLSPX_tcfniq"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    # Test the model\n",
    "    multi.cuda().eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data in loaders['test']:\n",
    "            test_output =  multi(data['img'].to(device))\n",
    "            (prd_male, prd_eye_glasses, prd_smiling, prd_wearing_hat, prd_young) = test_output\n",
    "            prd_male = torch.max(prd_male, 1)[1]\n",
    "            prd_eye_glasses = torch.max(prd_eye_glasses, 1)[1]\n",
    "            prd_smiling = torch.max(prd_smiling, 1)[1]\n",
    "            prd_wearing_hat = torch.max(prd_wearing_hat, 1)[1]\n",
    "            prd_young = torch.max(prd_young, 1)[1]\n",
    "\n",
    "            accuracy1 = (prd_male == data['male'].cuda()).sum().item() / float(data['male'].cuda().shape[0])\n",
    "            accuracy2 = (prd_eye_glasses == data['eye_glasses'].cuda()).sum().item() / float(data['eye_glasses'].cuda().shape[0])\n",
    "            accuracy3 = (prd_smiling == data['smiling'].cuda()).sum().item() / float(data['smiling'].cuda().shape[0])\n",
    "            accuracy4 = (prd_wearing_hat == data['wearing_hat'].cuda()).sum().item() / float(data['wearing_hat'].cuda().shape[0])\n",
    "            accuracy5 = (prd_young == data['young'].cuda()).sum().item() / float(data['young'].cuda().shape[0])\n",
    "        \n",
    "        print(\"Test Accuracy of the model on test images for Male: \", accuracy1*100)\n",
    "        print(\"Test Accuracy of the model on test images for Sun Glasses: \", accuracy2*100)\n",
    "        print(\"Test Accuracy of the model on test images for Similing: \", accuracy3*100)\n",
    "        print(\"Test Accuracy of the model on test images for Wearing Hat: \", accuracy4*100)\n",
    "        print(\"Test Accuracy of the model on test images for Young: \", accuracy5*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "web9rYOmxVPL",
    "outputId": "75be27f7-ff2d-4ee7-85d0-031d9794453c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on test images for Male:  61.111111111111114\n",
      "Test Accuracy of the model on test images for Sun Glasses:  91.66666666666666\n",
      "Test Accuracy of the model on test images for Similing:  50.0\n",
      "Test Accuracy of the model on test images for Wearing Hat:  94.44444444444444\n",
      "Test Accuracy of the model on test images for Young:  26.38888888888889\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of code.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

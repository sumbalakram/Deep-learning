{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "27829cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.utils import shuffle\n",
    "from torch.autograd import Variable\n",
    "from time import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0304606d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 1925 files [00:02, 895.59 files/s]\n"
     ]
    }
   ],
   "source": [
    "import splitfolders\n",
    "splitfolders.ratio('D:/MSDS/3rd Sem/ML/Assignment/CelebA/CelebA/dataset2', output=\"output2\", seed=1337, ratio=(.8, .2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d9f6f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "                transforms.Resize((64,64)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=45),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                                mean=[0.5, 0.5,0.5],\n",
    "                                std=[0.5, 0.5,0.5])])\n",
    " \n",
    "test_transform =transforms.Compose([\n",
    "                transforms.Resize((64, 64)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                                mean=[0.5, 0.5,0.5],\n",
    "                                std=[0.5, 0.5,0.5])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "caaa97f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_load(train_transform,test_transform,batch):\n",
    "  # Loading Datasets using ImageFolder\n",
    "  train_dataset = ImageFolder('D:/MSDS/3rd Sem/ML/Assignment/output2/train' , transform = train_transform) \n",
    "  print(len(train_dataset), 'Train Data')\n",
    "  \n",
    "  validation_dataset = ImageFolder('D:/MSDS/3rd Sem/ML/Assignment/output2/val' , transform = test_transform)\n",
    "  print(len(validation_dataset) , \"validation Data\")\n",
    "  \n",
    " \n",
    "\n",
    "  # Train and Test dataloaders\n",
    "  train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch, shuffle=True)\n",
    "  val_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch)\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "  return train_loader,val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8487709a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-2bb4d69f5009>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m datagen = ImageDataGenerator(        \n\u001b[0;32m      5\u001b[0m         \u001b[0mrotation_range\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage import io\n",
    "datagen = ImageDataGenerator(        \n",
    "        rotation_range = 40,\n",
    "        shear_range = 0.2,\n",
    "        zoom_range = 0.2,\n",
    "        horizontal_flip = True,\n",
    "        brightness_range = (0.5, 1.5))\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "image_directory = r'D:/MSDS/3rd Sem/ML/Assignment/output2/train'\n",
    "dataset = []\n",
    "my_images = os.listdir(image_directory)\n",
    "for i, image_name in enumerate(my_images):    \n",
    "    if (image_name.split('.')[1] == 'jpg'):        \n",
    "        image = io.imread(image_directory + image_name)        \n",
    "        image = Image.fromarray(image, 'RGB')        \n",
    "        image = image.resize((178,218)) \n",
    "        dataset.append(np.array(image))\n",
    "x = np.array(dataset)\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=16,\n",
    "                          save_to_dir= r'D:/MSDS/3rd Sem/ML/Assignment/output2/aug_train',\n",
    "                          save_format='jpg'):    \n",
    "    i += 1    \n",
    "    if i > 50:        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e92e25f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1515 Train Data\n",
      "410 validation Data\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "\n",
    "train_loader,val_loader = dataset_load(train_transform,test_transform,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a4bc5820",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # calculate same padding:\n",
    "        # (w - k + 2*p)/s + 1 = o\n",
    "        # => p = (s(o-1) - w + k)/2\n",
    "        \n",
    "        # 28x28x1 => 28x28x8\n",
    "        self.conv_1 = torch.nn.Conv2d(in_channels=3,\n",
    "                                      out_channels=16,\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding=1) # (1(28-1) - 28 + 3) / 2 = 1\n",
    "        # 28x28x8 => 14x14x8\n",
    "        self.pool_1 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                         stride=(2, 2),\n",
    "                                         padding=0) # (2(14-1) - 28 + 2) = 0                                       \n",
    "        # 14x14x8 => 14x14x16\n",
    "        self.conv_2 = torch.nn.Conv2d(in_channels=16,\n",
    "                                      out_channels=16,\n",
    "                                      kernel_size=(3, 3),\n",
    "                                      stride=(1, 1),\n",
    "                                      padding=1) # (1(14-1) - 14 + 3) / 2 = 1                 \n",
    "        # 14x14x16 => 7x7x16                             \n",
    "        self.pool_2 = torch.nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                                         stride=(2, 2),\n",
    "                                         padding=0) # (2(7-1) - 14 + 2) = 0\n",
    "\n",
    "        self.linear_1 = torch.nn.Linear(16*16*16, num_classes)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv_1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool_1(out)\n",
    "\n",
    "        out = self.conv_2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool_2(out)\n",
    "        \n",
    "        logits = self.linear_1(out.view(-1, 16*16*16))\n",
    "        probas = F.log_softmax(logits, dim=1)\n",
    "        \n",
    "        return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ab610a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet(100)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "99c6fb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/500 | Batch 000/024 | Cost: 4.6057\n",
      "Epoch: 001/500 training accuracy: 2.11%\n",
      "Epoch: 002/500 | Batch 000/024 | Cost: 4.5824\n",
      "Epoch: 002/500 training accuracy: 2.31%\n",
      "Epoch: 003/500 | Batch 000/024 | Cost: 4.5834\n",
      "Epoch: 003/500 training accuracy: 2.77%\n",
      "Epoch: 004/500 | Batch 000/024 | Cost: 4.5734\n",
      "Epoch: 004/500 training accuracy: 3.04%\n",
      "Epoch: 005/500 | Batch 000/024 | Cost: 4.5378\n",
      "Epoch: 005/500 training accuracy: 2.84%\n",
      "Epoch: 006/500 | Batch 000/024 | Cost: 4.5292\n",
      "Epoch: 006/500 training accuracy: 3.76%\n",
      "Epoch: 007/500 | Batch 000/024 | Cost: 4.4486\n",
      "Epoch: 007/500 training accuracy: 4.16%\n",
      "Epoch: 008/500 | Batch 000/024 | Cost: 4.5084\n",
      "Epoch: 008/500 training accuracy: 3.76%\n",
      "Epoch: 009/500 | Batch 000/024 | Cost: 4.4636\n",
      "Epoch: 009/500 training accuracy: 3.37%\n",
      "Epoch: 010/500 | Batch 000/024 | Cost: 4.5306\n",
      "Epoch: 010/500 training accuracy: 3.63%\n",
      "Epoch: 011/500 | Batch 000/024 | Cost: 4.3949\n",
      "Epoch: 011/500 training accuracy: 4.49%\n",
      "Epoch: 012/500 | Batch 000/024 | Cost: 4.2839\n",
      "Epoch: 012/500 training accuracy: 4.55%\n",
      "Epoch: 013/500 | Batch 000/024 | Cost: 4.3376\n",
      "Epoch: 013/500 training accuracy: 5.08%\n",
      "Epoch: 014/500 | Batch 000/024 | Cost: 4.4386\n",
      "Epoch: 014/500 training accuracy: 5.21%\n",
      "Epoch: 015/500 | Batch 000/024 | Cost: 4.2106\n",
      "Epoch: 015/500 training accuracy: 5.81%\n",
      "Epoch: 016/500 | Batch 000/024 | Cost: 4.2629\n",
      "Epoch: 016/500 training accuracy: 6.86%\n",
      "Epoch: 017/500 | Batch 000/024 | Cost: 4.1510\n",
      "Epoch: 017/500 training accuracy: 6.14%\n",
      "Epoch: 018/500 | Batch 000/024 | Cost: 4.2669\n",
      "Epoch: 018/500 training accuracy: 7.06%\n",
      "Epoch: 019/500 | Batch 000/024 | Cost: 4.2229\n",
      "Epoch: 019/500 training accuracy: 8.45%\n",
      "Epoch: 020/500 | Batch 000/024 | Cost: 4.1726\n",
      "Epoch: 020/500 training accuracy: 7.92%\n",
      "Epoch: 021/500 | Batch 000/024 | Cost: 3.9892\n",
      "Epoch: 021/500 training accuracy: 7.79%\n",
      "Epoch: 022/500 | Batch 000/024 | Cost: 3.9674\n",
      "Epoch: 022/500 training accuracy: 8.65%\n",
      "Epoch: 023/500 | Batch 000/024 | Cost: 4.1354\n",
      "Epoch: 023/500 training accuracy: 8.45%\n",
      "Epoch: 024/500 | Batch 000/024 | Cost: 3.9193\n",
      "Epoch: 024/500 training accuracy: 9.04%\n",
      "Epoch: 025/500 | Batch 000/024 | Cost: 3.8726\n",
      "Epoch: 025/500 training accuracy: 9.97%\n",
      "Epoch: 026/500 | Batch 000/024 | Cost: 3.9175\n",
      "Epoch: 026/500 training accuracy: 8.51%\n",
      "Epoch: 027/500 | Batch 000/024 | Cost: 3.8334\n",
      "Epoch: 027/500 training accuracy: 9.77%\n",
      "Epoch: 028/500 | Batch 000/024 | Cost: 3.6622\n",
      "Epoch: 028/500 training accuracy: 10.56%\n",
      "Epoch: 029/500 | Batch 000/024 | Cost: 3.6751\n",
      "Epoch: 029/500 training accuracy: 9.97%\n",
      "Epoch: 030/500 | Batch 000/024 | Cost: 3.9898\n",
      "Epoch: 030/500 training accuracy: 10.96%\n",
      "Epoch: 031/500 | Batch 000/024 | Cost: 3.9238\n",
      "Epoch: 031/500 training accuracy: 12.15%\n",
      "Epoch: 032/500 | Batch 000/024 | Cost: 3.8415\n",
      "Epoch: 032/500 training accuracy: 14.72%\n",
      "Epoch: 033/500 | Batch 000/024 | Cost: 3.8526\n",
      "Epoch: 033/500 training accuracy: 13.33%\n",
      "Epoch: 034/500 | Batch 000/024 | Cost: 4.0529\n",
      "Epoch: 034/500 training accuracy: 12.15%\n",
      "Epoch: 035/500 | Batch 000/024 | Cost: 3.5913\n",
      "Epoch: 035/500 training accuracy: 11.62%\n",
      "Epoch: 036/500 | Batch 000/024 | Cost: 3.8076\n",
      "Epoch: 036/500 training accuracy: 11.75%\n",
      "Epoch: 037/500 | Batch 000/024 | Cost: 3.6667\n",
      "Epoch: 037/500 training accuracy: 14.13%\n",
      "Epoch: 038/500 | Batch 000/024 | Cost: 3.6002\n",
      "Epoch: 038/500 training accuracy: 13.14%\n",
      "Epoch: 039/500 | Batch 000/024 | Cost: 3.5772\n",
      "Epoch: 039/500 training accuracy: 13.99%\n",
      "Epoch: 040/500 | Batch 000/024 | Cost: 3.8312\n",
      "Epoch: 040/500 training accuracy: 16.50%\n",
      "Epoch: 041/500 | Batch 000/024 | Cost: 3.4966\n",
      "Epoch: 041/500 training accuracy: 14.06%\n",
      "Epoch: 042/500 | Batch 000/024 | Cost: 3.4518\n",
      "Epoch: 042/500 training accuracy: 12.34%\n",
      "Epoch: 043/500 | Batch 000/024 | Cost: 3.7525\n",
      "Epoch: 043/500 training accuracy: 15.51%\n",
      "Epoch: 044/500 | Batch 000/024 | Cost: 3.4358\n",
      "Epoch: 044/500 training accuracy: 14.65%\n",
      "Epoch: 045/500 | Batch 000/024 | Cost: 3.3393\n",
      "Epoch: 045/500 training accuracy: 16.30%\n",
      "Epoch: 046/500 | Batch 000/024 | Cost: 3.3382\n",
      "Epoch: 046/500 training accuracy: 16.50%\n",
      "Epoch: 047/500 | Batch 000/024 | Cost: 3.2962\n",
      "Epoch: 047/500 training accuracy: 15.18%\n",
      "Epoch: 048/500 | Batch 000/024 | Cost: 3.4237\n",
      "Epoch: 048/500 training accuracy: 18.35%\n",
      "Epoch: 049/500 | Batch 000/024 | Cost: 3.7807\n",
      "Epoch: 049/500 training accuracy: 19.08%\n",
      "Epoch: 050/500 | Batch 000/024 | Cost: 3.3743\n",
      "Epoch: 050/500 training accuracy: 17.43%\n",
      "Epoch: 051/500 | Batch 000/024 | Cost: 3.1753\n",
      "Epoch: 051/500 training accuracy: 16.24%\n",
      "Epoch: 052/500 | Batch 000/024 | Cost: 3.5396\n",
      "Epoch: 052/500 training accuracy: 19.34%\n",
      "Epoch: 053/500 | Batch 000/024 | Cost: 3.3006\n",
      "Epoch: 053/500 training accuracy: 17.56%\n",
      "Epoch: 054/500 | Batch 000/024 | Cost: 3.4890\n",
      "Epoch: 054/500 training accuracy: 15.91%\n",
      "Epoch: 055/500 | Batch 000/024 | Cost: 3.4184\n",
      "Epoch: 055/500 training accuracy: 20.33%\n",
      "Epoch: 056/500 | Batch 000/024 | Cost: 3.7654\n",
      "Epoch: 056/500 training accuracy: 19.21%\n",
      "Epoch: 057/500 | Batch 000/024 | Cost: 3.0316\n",
      "Epoch: 057/500 training accuracy: 21.25%\n",
      "Epoch: 058/500 | Batch 000/024 | Cost: 3.2144\n",
      "Epoch: 058/500 training accuracy: 20.46%\n",
      "Epoch: 059/500 | Batch 000/024 | Cost: 3.1498\n",
      "Epoch: 059/500 training accuracy: 19.93%\n",
      "Epoch: 060/500 | Batch 000/024 | Cost: 3.4785\n",
      "Epoch: 060/500 training accuracy: 22.05%\n",
      "Epoch: 061/500 | Batch 000/024 | Cost: 3.0954\n",
      "Epoch: 061/500 training accuracy: 20.00%\n",
      "Epoch: 062/500 | Batch 000/024 | Cost: 3.0437\n",
      "Epoch: 062/500 training accuracy: 20.53%\n",
      "Epoch: 063/500 | Batch 000/024 | Cost: 2.9832\n",
      "Epoch: 063/500 training accuracy: 19.67%\n",
      "Epoch: 064/500 | Batch 000/024 | Cost: 3.3319\n",
      "Epoch: 064/500 training accuracy: 23.37%\n",
      "Epoch: 065/500 | Batch 000/024 | Cost: 3.1174\n",
      "Epoch: 065/500 training accuracy: 22.31%\n",
      "Epoch: 066/500 | Batch 000/024 | Cost: 3.1241\n",
      "Epoch: 066/500 training accuracy: 23.56%\n",
      "Epoch: 067/500 | Batch 000/024 | Cost: 3.2509\n",
      "Epoch: 067/500 training accuracy: 23.30%\n",
      "Epoch: 068/500 | Batch 000/024 | Cost: 3.3263\n",
      "Epoch: 068/500 training accuracy: 24.29%\n",
      "Epoch: 069/500 | Batch 000/024 | Cost: 2.9911\n",
      "Epoch: 069/500 training accuracy: 22.57%\n",
      "Epoch: 070/500 | Batch 000/024 | Cost: 3.1610\n",
      "Epoch: 070/500 training accuracy: 22.51%\n",
      "Epoch: 071/500 | Batch 000/024 | Cost: 3.0032\n",
      "Epoch: 071/500 training accuracy: 23.83%\n",
      "Epoch: 072/500 | Batch 000/024 | Cost: 2.8500\n",
      "Epoch: 072/500 training accuracy: 23.04%\n",
      "Epoch: 073/500 | Batch 000/024 | Cost: 3.3599\n",
      "Epoch: 073/500 training accuracy: 23.76%\n",
      "Epoch: 074/500 | Batch 000/024 | Cost: 2.8562\n",
      "Epoch: 074/500 training accuracy: 23.70%\n",
      "Epoch: 075/500 | Batch 000/024 | Cost: 3.3233\n",
      "Epoch: 075/500 training accuracy: 25.41%\n",
      "Epoch: 076/500 | Batch 000/024 | Cost: 2.9018\n",
      "Epoch: 076/500 training accuracy: 24.55%\n",
      "Epoch: 077/500 | Batch 000/024 | Cost: 2.9285\n",
      "Epoch: 077/500 training accuracy: 23.23%\n",
      "Epoch: 078/500 | Batch 000/024 | Cost: 3.1867\n",
      "Epoch: 078/500 training accuracy: 26.40%\n",
      "Epoch: 079/500 | Batch 000/024 | Cost: 3.0030\n",
      "Epoch: 079/500 training accuracy: 23.10%\n",
      "Epoch: 080/500 | Batch 000/024 | Cost: 3.0764\n",
      "Epoch: 080/500 training accuracy: 24.42%\n",
      "Epoch: 081/500 | Batch 000/024 | Cost: 2.8237\n",
      "Epoch: 081/500 training accuracy: 22.57%\n",
      "Epoch: 082/500 | Batch 000/024 | Cost: 3.0751\n",
      "Epoch: 082/500 training accuracy: 24.88%\n",
      "Epoch: 083/500 | Batch 000/024 | Cost: 2.8317\n",
      "Epoch: 083/500 training accuracy: 28.38%\n",
      "Epoch: 084/500 | Batch 000/024 | Cost: 3.1083\n",
      "Epoch: 084/500 training accuracy: 27.00%\n",
      "Epoch: 085/500 | Batch 000/024 | Cost: 2.9278\n",
      "Epoch: 085/500 training accuracy: 27.00%\n",
      "Epoch: 086/500 | Batch 000/024 | Cost: 3.0559\n",
      "Epoch: 086/500 training accuracy: 28.05%\n",
      "Epoch: 087/500 | Batch 000/024 | Cost: 2.8340\n",
      "Epoch: 087/500 training accuracy: 26.40%\n",
      "Epoch: 088/500 | Batch 000/024 | Cost: 3.1896\n",
      "Epoch: 088/500 training accuracy: 27.39%\n",
      "Epoch: 089/500 | Batch 000/024 | Cost: 2.9955\n",
      "Epoch: 089/500 training accuracy: 24.69%\n",
      "Epoch: 090/500 | Batch 000/024 | Cost: 2.7032\n",
      "Epoch: 090/500 training accuracy: 25.15%\n",
      "Epoch: 091/500 | Batch 000/024 | Cost: 2.9038\n",
      "Epoch: 091/500 training accuracy: 27.46%\n",
      "Epoch: 092/500 | Batch 000/024 | Cost: 2.6327\n",
      "Epoch: 092/500 training accuracy: 27.52%\n",
      "Epoch: 093/500 | Batch 000/024 | Cost: 3.1819\n",
      "Epoch: 093/500 training accuracy: 21.58%\n",
      "Epoch: 094/500 | Batch 000/024 | Cost: 2.7691\n",
      "Epoch: 094/500 training accuracy: 29.37%\n",
      "Epoch: 095/500 | Batch 000/024 | Cost: 2.7131\n",
      "Epoch: 095/500 training accuracy: 28.51%\n",
      "Epoch: 096/500 | Batch 000/024 | Cost: 2.7298\n",
      "Epoch: 096/500 training accuracy: 20.53%\n",
      "Epoch: 097/500 | Batch 000/024 | Cost: 3.3421\n",
      "Epoch: 097/500 training accuracy: 31.55%\n",
      "Epoch: 098/500 | Batch 000/024 | Cost: 2.9181\n",
      "Epoch: 098/500 training accuracy: 29.37%\n",
      "Epoch: 099/500 | Batch 000/024 | Cost: 2.7898\n",
      "Epoch: 099/500 training accuracy: 27.52%\n",
      "Epoch: 100/500 | Batch 000/024 | Cost: 2.9612\n",
      "Epoch: 100/500 training accuracy: 29.97%\n",
      "Epoch: 101/500 | Batch 000/024 | Cost: 2.5544\n",
      "Epoch: 101/500 training accuracy: 31.16%\n",
      "Epoch: 102/500 | Batch 000/024 | Cost: 2.8885\n",
      "Epoch: 102/500 training accuracy: 27.99%\n",
      "Epoch: 103/500 | Batch 000/024 | Cost: 2.9681\n",
      "Epoch: 103/500 training accuracy: 30.96%\n",
      "Epoch: 104/500 | Batch 000/024 | Cost: 2.5962\n",
      "Epoch: 104/500 training accuracy: 29.77%\n",
      "Epoch: 105/500 | Batch 000/024 | Cost: 2.7462\n",
      "Epoch: 105/500 training accuracy: 33.14%\n",
      "Epoch: 106/500 | Batch 000/024 | Cost: 2.9370\n",
      "Epoch: 106/500 training accuracy: 30.36%\n",
      "Epoch: 107/500 | Batch 000/024 | Cost: 2.6128\n",
      "Epoch: 107/500 training accuracy: 34.39%\n",
      "Epoch: 108/500 | Batch 000/024 | Cost: 2.6497\n",
      "Epoch: 108/500 training accuracy: 32.48%\n",
      "Epoch: 109/500 | Batch 000/024 | Cost: 2.5143\n",
      "Epoch: 109/500 training accuracy: 31.88%\n",
      "Epoch: 110/500 | Batch 000/024 | Cost: 2.3237\n",
      "Epoch: 110/500 training accuracy: 31.82%\n",
      "Epoch: 111/500 | Batch 000/024 | Cost: 2.6108\n",
      "Epoch: 111/500 training accuracy: 29.64%\n",
      "Epoch: 112/500 | Batch 000/024 | Cost: 2.8439\n",
      "Epoch: 112/500 training accuracy: 33.33%\n",
      "Epoch: 113/500 | Batch 000/024 | Cost: 2.6462\n",
      "Epoch: 113/500 training accuracy: 33.99%\n",
      "Epoch: 114/500 | Batch 000/024 | Cost: 2.7444\n",
      "Epoch: 114/500 training accuracy: 32.54%\n",
      "Epoch: 115/500 | Batch 000/024 | Cost: 2.7917\n",
      "Epoch: 115/500 training accuracy: 28.25%\n",
      "Epoch: 116/500 | Batch 000/024 | Cost: 3.1108\n",
      "Epoch: 116/500 training accuracy: 34.79%\n",
      "Epoch: 117/500 | Batch 000/024 | Cost: 2.5910\n",
      "Epoch: 117/500 training accuracy: 34.59%\n",
      "Epoch: 118/500 | Batch 000/024 | Cost: 2.4156\n",
      "Epoch: 118/500 training accuracy: 37.43%\n",
      "Epoch: 119/500 | Batch 000/024 | Cost: 2.8150\n",
      "Epoch: 119/500 training accuracy: 34.39%\n",
      "Epoch: 120/500 | Batch 000/024 | Cost: 2.3471\n",
      "Epoch: 120/500 training accuracy: 32.08%\n",
      "Epoch: 121/500 | Batch 000/024 | Cost: 2.7151\n",
      "Epoch: 121/500 training accuracy: 36.24%\n",
      "Epoch: 122/500 | Batch 000/024 | Cost: 2.2212\n",
      "Epoch: 122/500 training accuracy: 36.11%\n",
      "Epoch: 123/500 | Batch 000/024 | Cost: 2.4690\n",
      "Epoch: 123/500 training accuracy: 36.50%\n",
      "Epoch: 124/500 | Batch 000/024 | Cost: 2.7888\n",
      "Epoch: 124/500 training accuracy: 38.02%\n",
      "Epoch: 125/500 | Batch 000/024 | Cost: 2.2545\n",
      "Epoch: 125/500 training accuracy: 39.21%\n",
      "Epoch: 126/500 | Batch 000/024 | Cost: 2.5299\n",
      "Epoch: 126/500 training accuracy: 36.11%\n",
      "Epoch: 127/500 | Batch 000/024 | Cost: 3.0915\n",
      "Epoch: 127/500 training accuracy: 33.66%\n",
      "Epoch: 128/500 | Batch 000/024 | Cost: 2.4932\n",
      "Epoch: 128/500 training accuracy: 34.39%\n",
      "Epoch: 129/500 | Batch 000/024 | Cost: 2.5148\n",
      "Epoch: 129/500 training accuracy: 36.24%\n",
      "Epoch: 130/500 | Batch 000/024 | Cost: 2.5758\n",
      "Epoch: 130/500 training accuracy: 31.35%\n",
      "Epoch: 131/500 | Batch 000/024 | Cost: 2.4217\n",
      "Epoch: 131/500 training accuracy: 34.26%\n",
      "Epoch: 132/500 | Batch 000/024 | Cost: 2.2350\n",
      "Epoch: 132/500 training accuracy: 33.53%\n",
      "Epoch: 133/500 | Batch 000/024 | Cost: 2.1116\n",
      "Epoch: 133/500 training accuracy: 37.82%\n",
      "Epoch: 134/500 | Batch 000/024 | Cost: 2.1935\n",
      "Epoch: 134/500 training accuracy: 35.84%\n",
      "Epoch: 135/500 | Batch 000/024 | Cost: 2.4610\n",
      "Epoch: 135/500 training accuracy: 37.29%\n",
      "Epoch: 136/500 | Batch 000/024 | Cost: 2.1020\n",
      "Epoch: 136/500 training accuracy: 40.33%\n",
      "Epoch: 137/500 | Batch 000/024 | Cost: 2.6891\n",
      "Epoch: 137/500 training accuracy: 39.47%\n",
      "Epoch: 138/500 | Batch 000/024 | Cost: 2.2240\n",
      "Epoch: 138/500 training accuracy: 36.77%\n",
      "Epoch: 139/500 | Batch 000/024 | Cost: 2.1876\n",
      "Epoch: 139/500 training accuracy: 39.54%\n",
      "Epoch: 140/500 | Batch 000/024 | Cost: 2.1429\n",
      "Epoch: 140/500 training accuracy: 39.74%\n",
      "Epoch: 141/500 | Batch 000/024 | Cost: 2.2702\n",
      "Epoch: 141/500 training accuracy: 37.10%\n",
      "Epoch: 142/500 | Batch 000/024 | Cost: 2.4981\n",
      "Epoch: 142/500 training accuracy: 35.64%\n",
      "Epoch: 143/500 | Batch 000/024 | Cost: 2.5376\n",
      "Epoch: 143/500 training accuracy: 40.26%\n",
      "Epoch: 144/500 | Batch 000/024 | Cost: 1.8415\n",
      "Epoch: 144/500 training accuracy: 43.37%\n",
      "Epoch: 145/500 | Batch 000/024 | Cost: 2.6486\n",
      "Epoch: 145/500 training accuracy: 40.86%\n",
      "Epoch: 146/500 | Batch 000/024 | Cost: 2.0247\n",
      "Epoch: 146/500 training accuracy: 43.17%\n",
      "Epoch: 147/500 | Batch 000/024 | Cost: 2.2095\n",
      "Epoch: 147/500 training accuracy: 43.23%\n",
      "Epoch: 148/500 | Batch 000/024 | Cost: 1.8328\n",
      "Epoch: 148/500 training accuracy: 42.11%\n",
      "Epoch: 149/500 | Batch 000/024 | Cost: 2.2521\n",
      "Epoch: 149/500 training accuracy: 41.72%\n",
      "Epoch: 150/500 | Batch 000/024 | Cost: 1.8297\n",
      "Epoch: 150/500 training accuracy: 38.88%\n",
      "Epoch: 151/500 | Batch 000/024 | Cost: 2.5322\n",
      "Epoch: 151/500 training accuracy: 43.17%\n",
      "Epoch: 152/500 | Batch 000/024 | Cost: 2.1306\n",
      "Epoch: 152/500 training accuracy: 42.38%\n",
      "Epoch: 153/500 | Batch 000/024 | Cost: 2.0216\n",
      "Epoch: 153/500 training accuracy: 42.71%\n",
      "Epoch: 154/500 | Batch 000/024 | Cost: 1.9718\n",
      "Epoch: 154/500 training accuracy: 48.38%\n",
      "Epoch: 155/500 | Batch 000/024 | Cost: 2.1969\n",
      "Epoch: 155/500 training accuracy: 43.96%\n",
      "Epoch: 156/500 | Batch 000/024 | Cost: 2.3542\n",
      "Epoch: 156/500 training accuracy: 45.68%\n",
      "Epoch: 157/500 | Batch 000/024 | Cost: 1.9568\n",
      "Epoch: 157/500 training accuracy: 41.32%\n",
      "Epoch: 158/500 | Batch 000/024 | Cost: 2.2220\n",
      "Epoch: 158/500 training accuracy: 40.20%\n",
      "Epoch: 159/500 | Batch 000/024 | Cost: 2.1781\n",
      "Epoch: 159/500 training accuracy: 47.92%\n",
      "Epoch: 160/500 | Batch 000/024 | Cost: 2.4753\n",
      "Epoch: 160/500 training accuracy: 44.62%\n",
      "Epoch: 161/500 | Batch 000/024 | Cost: 2.0229\n",
      "Epoch: 161/500 training accuracy: 43.23%\n",
      "Epoch: 162/500 | Batch 000/024 | Cost: 2.2772\n",
      "Epoch: 162/500 training accuracy: 47.46%\n",
      "Epoch: 163/500 | Batch 000/024 | Cost: 2.2976\n",
      "Epoch: 163/500 training accuracy: 44.22%\n",
      "Epoch: 164/500 | Batch 000/024 | Cost: 2.0109\n",
      "Epoch: 164/500 training accuracy: 46.53%\n",
      "Epoch: 165/500 | Batch 000/024 | Cost: 1.9284\n",
      "Epoch: 165/500 training accuracy: 47.85%\n",
      "Epoch: 166/500 | Batch 000/024 | Cost: 1.7420\n",
      "Epoch: 166/500 training accuracy: 48.78%\n",
      "Epoch: 167/500 | Batch 000/024 | Cost: 1.7011\n",
      "Epoch: 167/500 training accuracy: 42.57%\n",
      "Epoch: 168/500 | Batch 000/024 | Cost: 1.6519\n",
      "Epoch: 168/500 training accuracy: 47.46%\n",
      "Epoch: 169/500 | Batch 000/024 | Cost: 2.2590\n",
      "Epoch: 169/500 training accuracy: 45.48%\n",
      "Epoch: 170/500 | Batch 000/024 | Cost: 1.9018\n",
      "Epoch: 170/500 training accuracy: 49.97%\n",
      "Epoch: 171/500 | Batch 000/024 | Cost: 2.0156\n",
      "Epoch: 171/500 training accuracy: 49.31%\n",
      "Epoch: 172/500 | Batch 000/024 | Cost: 1.8621\n",
      "Epoch: 172/500 training accuracy: 49.90%\n",
      "Epoch: 173/500 | Batch 000/024 | Cost: 1.6599\n",
      "Epoch: 173/500 training accuracy: 50.89%\n",
      "Epoch: 174/500 | Batch 000/024 | Cost: 1.7474\n",
      "Epoch: 174/500 training accuracy: 47.52%\n",
      "Epoch: 175/500 | Batch 000/024 | Cost: 2.0935\n",
      "Epoch: 175/500 training accuracy: 49.90%\n",
      "Epoch: 176/500 | Batch 000/024 | Cost: 1.6969\n",
      "Epoch: 176/500 training accuracy: 52.94%\n",
      "Epoch: 177/500 | Batch 000/024 | Cost: 1.8629\n",
      "Epoch: 177/500 training accuracy: 50.03%\n",
      "Epoch: 178/500 | Batch 000/024 | Cost: 1.7245\n",
      "Epoch: 178/500 training accuracy: 49.37%\n",
      "Epoch: 179/500 | Batch 000/024 | Cost: 1.9518\n",
      "Epoch: 179/500 training accuracy: 51.02%\n",
      "Epoch: 180/500 | Batch 000/024 | Cost: 2.0237\n",
      "Epoch: 180/500 training accuracy: 48.38%\n",
      "Epoch: 181/500 | Batch 000/024 | Cost: 1.9365\n",
      "Epoch: 181/500 training accuracy: 50.23%\n",
      "Epoch: 182/500 | Batch 000/024 | Cost: 1.6170\n",
      "Epoch: 182/500 training accuracy: 52.21%\n",
      "Epoch: 183/500 | Batch 000/024 | Cost: 1.6901\n",
      "Epoch: 183/500 training accuracy: 52.67%\n",
      "Epoch: 184/500 | Batch 000/024 | Cost: 1.8527\n",
      "Epoch: 184/500 training accuracy: 56.57%\n",
      "Epoch: 185/500 | Batch 000/024 | Cost: 2.0771\n",
      "Epoch: 185/500 training accuracy: 50.56%\n",
      "Epoch: 186/500 | Batch 000/024 | Cost: 1.6612\n",
      "Epoch: 186/500 training accuracy: 54.85%\n",
      "Epoch: 187/500 | Batch 000/024 | Cost: 1.7315\n",
      "Epoch: 187/500 training accuracy: 52.21%\n",
      "Epoch: 188/500 | Batch 000/024 | Cost: 1.8756\n",
      "Epoch: 188/500 training accuracy: 54.06%\n",
      "Epoch: 189/500 | Batch 000/024 | Cost: 1.8318\n",
      "Epoch: 189/500 training accuracy: 49.77%\n",
      "Epoch: 190/500 | Batch 000/024 | Cost: 2.0909\n",
      "Epoch: 190/500 training accuracy: 55.25%\n",
      "Epoch: 191/500 | Batch 000/024 | Cost: 1.6318\n",
      "Epoch: 191/500 training accuracy: 53.93%\n",
      "Epoch: 192/500 | Batch 000/024 | Cost: 1.7009\n",
      "Epoch: 192/500 training accuracy: 55.84%\n",
      "Epoch: 193/500 | Batch 000/024 | Cost: 1.9627\n",
      "Epoch: 193/500 training accuracy: 53.14%\n",
      "Epoch: 194/500 | Batch 000/024 | Cost: 1.8071\n",
      "Epoch: 194/500 training accuracy: 56.11%\n",
      "Epoch: 195/500 | Batch 000/024 | Cost: 1.8993\n",
      "Epoch: 195/500 training accuracy: 55.05%\n",
      "Epoch: 196/500 | Batch 000/024 | Cost: 1.4876\n",
      "Epoch: 196/500 training accuracy: 56.96%\n",
      "Epoch: 197/500 | Batch 000/024 | Cost: 1.5615\n",
      "Epoch: 197/500 training accuracy: 54.32%\n",
      "Epoch: 198/500 | Batch 000/024 | Cost: 1.7686\n",
      "Epoch: 198/500 training accuracy: 55.51%\n",
      "Epoch: 199/500 | Batch 000/024 | Cost: 1.5343\n",
      "Epoch: 199/500 training accuracy: 56.17%\n",
      "Epoch: 200/500 | Batch 000/024 | Cost: 1.4852\n",
      "Epoch: 200/500 training accuracy: 57.10%\n",
      "Epoch: 201/500 | Batch 000/024 | Cost: 1.3449\n",
      "Epoch: 201/500 training accuracy: 56.50%\n",
      "Epoch: 202/500 | Batch 000/024 | Cost: 1.6779\n",
      "Epoch: 202/500 training accuracy: 53.60%\n",
      "Epoch: 203/500 | Batch 000/024 | Cost: 1.3907\n",
      "Epoch: 203/500 training accuracy: 52.15%\n",
      "Epoch: 204/500 | Batch 000/024 | Cost: 2.0047\n",
      "Epoch: 204/500 training accuracy: 54.59%\n",
      "Epoch: 205/500 | Batch 000/024 | Cost: 1.6461\n",
      "Epoch: 205/500 training accuracy: 58.42%\n",
      "Epoch: 206/500 | Batch 000/024 | Cost: 1.6031\n",
      "Epoch: 206/500 training accuracy: 54.72%\n",
      "Epoch: 207/500 | Batch 000/024 | Cost: 1.5595\n",
      "Epoch: 207/500 training accuracy: 60.79%\n",
      "Epoch: 208/500 | Batch 000/024 | Cost: 1.5567\n",
      "Epoch: 208/500 training accuracy: 62.97%\n",
      "Epoch: 209/500 | Batch 000/024 | Cost: 1.4603\n",
      "Epoch: 209/500 training accuracy: 58.55%\n",
      "Epoch: 210/500 | Batch 000/024 | Cost: 1.6785\n",
      "Epoch: 210/500 training accuracy: 53.86%\n",
      "Epoch: 211/500 | Batch 000/024 | Cost: 1.4696\n",
      "Epoch: 211/500 training accuracy: 56.24%\n",
      "Epoch: 212/500 | Batch 000/024 | Cost: 1.4679\n",
      "Epoch: 212/500 training accuracy: 59.74%\n",
      "Epoch: 213/500 | Batch 000/024 | Cost: 1.5602\n",
      "Epoch: 213/500 training accuracy: 56.24%\n",
      "Epoch: 214/500 | Batch 000/024 | Cost: 1.2532\n",
      "Epoch: 214/500 training accuracy: 54.98%\n",
      "Epoch: 215/500 | Batch 000/024 | Cost: 1.4668\n",
      "Epoch: 215/500 training accuracy: 60.20%\n",
      "Epoch: 216/500 | Batch 000/024 | Cost: 1.3256\n",
      "Epoch: 216/500 training accuracy: 59.93%\n",
      "Epoch: 217/500 | Batch 000/024 | Cost: 1.7053\n",
      "Epoch: 217/500 training accuracy: 57.56%\n",
      "Epoch: 218/500 | Batch 000/024 | Cost: 1.5405\n",
      "Epoch: 218/500 training accuracy: 56.37%\n",
      "Epoch: 219/500 | Batch 000/024 | Cost: 1.2850\n",
      "Epoch: 219/500 training accuracy: 61.45%\n",
      "Epoch: 220/500 | Batch 000/024 | Cost: 1.4686\n",
      "Epoch: 220/500 training accuracy: 57.76%\n",
      "Epoch: 221/500 | Batch 000/024 | Cost: 2.0981\n",
      "Epoch: 221/500 training accuracy: 63.70%\n",
      "Epoch: 222/500 | Batch 000/024 | Cost: 1.5198\n",
      "Epoch: 222/500 training accuracy: 60.66%\n",
      "Epoch: 223/500 | Batch 000/024 | Cost: 1.3226\n",
      "Epoch: 223/500 training accuracy: 62.24%\n",
      "Epoch: 224/500 | Batch 000/024 | Cost: 1.2556\n",
      "Epoch: 224/500 training accuracy: 57.89%\n",
      "Epoch: 225/500 | Batch 000/024 | Cost: 1.7553\n",
      "Epoch: 225/500 training accuracy: 56.63%\n",
      "Epoch: 226/500 | Batch 000/024 | Cost: 1.4385\n",
      "Epoch: 226/500 training accuracy: 61.52%\n",
      "Epoch: 227/500 | Batch 000/024 | Cost: 1.6136\n",
      "Epoch: 227/500 training accuracy: 57.89%\n",
      "Epoch: 228/500 | Batch 000/024 | Cost: 1.8388\n",
      "Epoch: 228/500 training accuracy: 58.61%\n",
      "Epoch: 229/500 | Batch 000/024 | Cost: 1.6430\n",
      "Epoch: 229/500 training accuracy: 62.31%\n",
      "Epoch: 230/500 | Batch 000/024 | Cost: 1.4123\n",
      "Epoch: 230/500 training accuracy: 61.25%\n",
      "Epoch: 231/500 | Batch 000/024 | Cost: 1.1401\n",
      "Epoch: 231/500 training accuracy: 62.11%\n",
      "Epoch: 232/500 | Batch 000/024 | Cost: 1.4318\n",
      "Epoch: 232/500 training accuracy: 58.28%\n",
      "Epoch: 233/500 | Batch 000/024 | Cost: 1.2887\n",
      "Epoch: 233/500 training accuracy: 60.07%\n",
      "Epoch: 234/500 | Batch 000/024 | Cost: 1.5472\n",
      "Epoch: 234/500 training accuracy: 62.84%\n",
      "Epoch: 235/500 | Batch 000/024 | Cost: 1.2924\n",
      "Epoch: 235/500 training accuracy: 67.19%\n",
      "Epoch: 236/500 | Batch 000/024 | Cost: 1.3072\n",
      "Epoch: 236/500 training accuracy: 64.09%\n",
      "Epoch: 237/500 | Batch 000/024 | Cost: 1.0673\n",
      "Epoch: 237/500 training accuracy: 64.42%\n",
      "Epoch: 238/500 | Batch 000/024 | Cost: 1.2137\n",
      "Epoch: 238/500 training accuracy: 62.84%\n",
      "Epoch: 239/500 | Batch 000/024 | Cost: 1.3493\n",
      "Epoch: 239/500 training accuracy: 65.74%\n",
      "Epoch: 240/500 | Batch 000/024 | Cost: 1.5093\n",
      "Epoch: 240/500 training accuracy: 62.31%\n",
      "Epoch: 241/500 | Batch 000/024 | Cost: 1.4077\n",
      "Epoch: 241/500 training accuracy: 61.98%\n",
      "Epoch: 242/500 | Batch 000/024 | Cost: 1.6411\n",
      "Epoch: 242/500 training accuracy: 61.39%\n",
      "Epoch: 243/500 | Batch 000/024 | Cost: 1.4418\n",
      "Epoch: 243/500 training accuracy: 59.34%\n",
      "Epoch: 244/500 | Batch 000/024 | Cost: 1.7138\n",
      "Epoch: 244/500 training accuracy: 65.68%\n",
      "Epoch: 245/500 | Batch 000/024 | Cost: 1.2902\n",
      "Epoch: 245/500 training accuracy: 64.69%\n",
      "Epoch: 246/500 | Batch 000/024 | Cost: 1.5707\n",
      "Epoch: 246/500 training accuracy: 63.70%\n",
      "Epoch: 247/500 | Batch 000/024 | Cost: 1.1721\n",
      "Epoch: 247/500 training accuracy: 64.82%\n",
      "Epoch: 248/500 | Batch 000/024 | Cost: 1.5690\n",
      "Epoch: 248/500 training accuracy: 64.75%\n",
      "Epoch: 249/500 | Batch 000/024 | Cost: 1.2480\n",
      "Epoch: 249/500 training accuracy: 66.47%\n",
      "Epoch: 250/500 | Batch 000/024 | Cost: 1.1639\n",
      "Epoch: 250/500 training accuracy: 68.65%\n",
      "Epoch: 251/500 | Batch 000/024 | Cost: 1.3092\n",
      "Epoch: 251/500 training accuracy: 64.69%\n",
      "Epoch: 252/500 | Batch 000/024 | Cost: 1.5559\n",
      "Epoch: 252/500 training accuracy: 67.19%\n",
      "Epoch: 253/500 | Batch 000/024 | Cost: 0.9098\n",
      "Epoch: 253/500 training accuracy: 66.27%\n",
      "Epoch: 254/500 | Batch 000/024 | Cost: 1.4260\n",
      "Epoch: 254/500 training accuracy: 66.86%\n",
      "Epoch: 255/500 | Batch 000/024 | Cost: 1.1007\n",
      "Epoch: 255/500 training accuracy: 66.53%\n",
      "Epoch: 256/500 | Batch 000/024 | Cost: 1.4313\n",
      "Epoch: 256/500 training accuracy: 68.78%\n",
      "Epoch: 257/500 | Batch 000/024 | Cost: 0.9354\n",
      "Epoch: 257/500 training accuracy: 71.75%\n",
      "Epoch: 258/500 | Batch 000/024 | Cost: 0.8242\n",
      "Epoch: 258/500 training accuracy: 66.93%\n",
      "Epoch: 259/500 | Batch 000/024 | Cost: 1.1544\n",
      "Epoch: 259/500 training accuracy: 66.86%\n",
      "Epoch: 260/500 | Batch 000/024 | Cost: 1.1122\n",
      "Epoch: 260/500 training accuracy: 69.50%\n",
      "Epoch: 261/500 | Batch 000/024 | Cost: 1.3171\n",
      "Epoch: 261/500 training accuracy: 65.08%\n",
      "Epoch: 262/500 | Batch 000/024 | Cost: 0.9900\n",
      "Epoch: 262/500 training accuracy: 56.90%\n",
      "Epoch: 263/500 | Batch 000/024 | Cost: 1.3873\n",
      "Epoch: 263/500 training accuracy: 70.69%\n",
      "Epoch: 264/500 | Batch 000/024 | Cost: 0.9448\n",
      "Epoch: 264/500 training accuracy: 67.33%\n",
      "Epoch: 265/500 | Batch 000/024 | Cost: 1.0920\n",
      "Epoch: 265/500 training accuracy: 69.31%\n",
      "Epoch: 266/500 | Batch 000/024 | Cost: 0.9685\n",
      "Epoch: 266/500 training accuracy: 67.66%\n",
      "Epoch: 267/500 | Batch 000/024 | Cost: 1.6502\n",
      "Epoch: 267/500 training accuracy: 69.50%\n",
      "Epoch: 268/500 | Batch 000/024 | Cost: 1.0476\n",
      "Epoch: 268/500 training accuracy: 70.17%\n",
      "Epoch: 269/500 | Batch 000/024 | Cost: 1.2125\n",
      "Epoch: 269/500 training accuracy: 70.36%\n",
      "Epoch: 270/500 | Batch 000/024 | Cost: 1.1034\n",
      "Epoch: 270/500 training accuracy: 71.35%\n",
      "Epoch: 271/500 | Batch 000/024 | Cost: 1.0275\n",
      "Epoch: 271/500 training accuracy: 63.30%\n",
      "Epoch: 272/500 | Batch 000/024 | Cost: 1.3169\n",
      "Epoch: 272/500 training accuracy: 74.52%\n",
      "Epoch: 273/500 | Batch 000/024 | Cost: 1.0142\n",
      "Epoch: 273/500 training accuracy: 71.16%\n",
      "Epoch: 274/500 | Batch 000/024 | Cost: 0.7570\n",
      "Epoch: 274/500 training accuracy: 69.37%\n",
      "Epoch: 275/500 | Batch 000/024 | Cost: 1.2738\n",
      "Epoch: 275/500 training accuracy: 69.77%\n",
      "Epoch: 276/500 | Batch 000/024 | Cost: 1.0280\n",
      "Epoch: 276/500 training accuracy: 72.15%\n",
      "Epoch: 277/500 | Batch 000/024 | Cost: 0.9762\n",
      "Epoch: 277/500 training accuracy: 73.66%\n",
      "Epoch: 278/500 | Batch 000/024 | Cost: 1.0490\n",
      "Epoch: 278/500 training accuracy: 70.23%\n",
      "Epoch: 279/500 | Batch 000/024 | Cost: 0.8492\n",
      "Epoch: 279/500 training accuracy: 66.86%\n",
      "Epoch: 280/500 | Batch 000/024 | Cost: 1.3826\n",
      "Epoch: 280/500 training accuracy: 73.00%\n",
      "Epoch: 281/500 | Batch 000/024 | Cost: 0.8198\n",
      "Epoch: 281/500 training accuracy: 64.55%\n",
      "Epoch: 282/500 | Batch 000/024 | Cost: 1.2118\n",
      "Epoch: 282/500 training accuracy: 64.29%\n",
      "Epoch: 283/500 | Batch 000/024 | Cost: 1.1902\n",
      "Epoch: 283/500 training accuracy: 72.21%\n",
      "Epoch: 284/500 | Batch 000/024 | Cost: 1.1364\n",
      "Epoch: 284/500 training accuracy: 73.27%\n",
      "Epoch: 285/500 | Batch 000/024 | Cost: 0.8779\n",
      "Epoch: 285/500 training accuracy: 71.68%\n",
      "Epoch: 286/500 | Batch 000/024 | Cost: 0.9511\n",
      "Epoch: 286/500 training accuracy: 64.95%\n",
      "Epoch: 287/500 | Batch 000/024 | Cost: 1.1873\n",
      "Epoch: 287/500 training accuracy: 68.84%\n",
      "Epoch: 288/500 | Batch 000/024 | Cost: 1.2344\n",
      "Epoch: 288/500 training accuracy: 75.25%\n",
      "Epoch: 289/500 | Batch 000/024 | Cost: 0.9057\n",
      "Epoch: 289/500 training accuracy: 69.50%\n",
      "Epoch: 290/500 | Batch 000/024 | Cost: 1.0284\n",
      "Epoch: 290/500 training accuracy: 70.30%\n",
      "Epoch: 291/500 | Batch 000/024 | Cost: 1.1501\n",
      "Epoch: 291/500 training accuracy: 77.10%\n",
      "Epoch: 292/500 | Batch 000/024 | Cost: 0.6528\n",
      "Epoch: 292/500 training accuracy: 73.40%\n",
      "Epoch: 293/500 | Batch 000/024 | Cost: 0.8121\n",
      "Epoch: 293/500 training accuracy: 73.47%\n",
      "Epoch: 294/500 | Batch 000/024 | Cost: 0.6323\n",
      "Epoch: 294/500 training accuracy: 75.91%\n",
      "Epoch: 295/500 | Batch 000/024 | Cost: 0.6056\n",
      "Epoch: 295/500 training accuracy: 75.05%\n",
      "Epoch: 296/500 | Batch 000/024 | Cost: 1.0058\n",
      "Epoch: 296/500 training accuracy: 73.86%\n",
      "Epoch: 297/500 | Batch 000/024 | Cost: 1.0070\n",
      "Epoch: 297/500 training accuracy: 74.65%\n",
      "Epoch: 298/500 | Batch 000/024 | Cost: 0.9826\n",
      "Epoch: 298/500 training accuracy: 72.94%\n",
      "Epoch: 299/500 | Batch 000/024 | Cost: 0.7242\n",
      "Epoch: 299/500 training accuracy: 76.17%\n",
      "Epoch: 300/500 | Batch 000/024 | Cost: 1.0293\n",
      "Epoch: 300/500 training accuracy: 75.38%\n",
      "Epoch: 301/500 | Batch 000/024 | Cost: 0.8234\n",
      "Epoch: 301/500 training accuracy: 63.83%\n",
      "Epoch: 302/500 | Batch 000/024 | Cost: 1.1214\n",
      "Epoch: 302/500 training accuracy: 73.00%\n",
      "Epoch: 303/500 | Batch 000/024 | Cost: 0.7905\n",
      "Epoch: 303/500 training accuracy: 76.24%\n",
      "Epoch: 304/500 | Batch 000/024 | Cost: 0.8290\n",
      "Epoch: 304/500 training accuracy: 73.60%\n",
      "Epoch: 305/500 | Batch 000/024 | Cost: 0.7846\n",
      "Epoch: 305/500 training accuracy: 74.98%\n",
      "Epoch: 306/500 | Batch 000/024 | Cost: 0.8492\n",
      "Epoch: 306/500 training accuracy: 76.57%\n",
      "Epoch: 307/500 | Batch 000/024 | Cost: 0.8777\n",
      "Epoch: 307/500 training accuracy: 75.25%\n",
      "Epoch: 308/500 | Batch 000/024 | Cost: 0.9458\n",
      "Epoch: 308/500 training accuracy: 77.89%\n",
      "Epoch: 309/500 | Batch 000/024 | Cost: 0.7950\n",
      "Epoch: 309/500 training accuracy: 76.04%\n",
      "Epoch: 310/500 | Batch 000/024 | Cost: 1.0422\n",
      "Epoch: 310/500 training accuracy: 75.45%\n",
      "Epoch: 311/500 | Batch 000/024 | Cost: 0.9249\n",
      "Epoch: 311/500 training accuracy: 78.42%\n",
      "Epoch: 312/500 | Batch 000/024 | Cost: 0.8462\n",
      "Epoch: 312/500 training accuracy: 75.51%\n",
      "Epoch: 313/500 | Batch 000/024 | Cost: 0.5672\n",
      "Epoch: 313/500 training accuracy: 71.49%\n",
      "Epoch: 314/500 | Batch 000/024 | Cost: 0.5947\n",
      "Epoch: 314/500 training accuracy: 78.88%\n",
      "Epoch: 315/500 | Batch 000/024 | Cost: 0.8607\n",
      "Epoch: 315/500 training accuracy: 75.78%\n",
      "Epoch: 316/500 | Batch 000/024 | Cost: 0.9998\n",
      "Epoch: 316/500 training accuracy: 80.40%\n",
      "Epoch: 317/500 | Batch 000/024 | Cost: 0.7966\n",
      "Epoch: 317/500 training accuracy: 74.59%\n",
      "Epoch: 318/500 | Batch 000/024 | Cost: 1.2198\n",
      "Epoch: 318/500 training accuracy: 76.77%\n",
      "Epoch: 319/500 | Batch 000/024 | Cost: 0.7423\n",
      "Epoch: 319/500 training accuracy: 69.57%\n",
      "Epoch: 320/500 | Batch 000/024 | Cost: 0.9123\n",
      "Epoch: 320/500 training accuracy: 77.43%\n",
      "Epoch: 321/500 | Batch 000/024 | Cost: 0.7576\n",
      "Epoch: 321/500 training accuracy: 75.25%\n",
      "Epoch: 322/500 | Batch 000/024 | Cost: 0.9763\n",
      "Epoch: 322/500 training accuracy: 73.47%\n",
      "Epoch: 323/500 | Batch 000/024 | Cost: 0.5720\n",
      "Epoch: 323/500 training accuracy: 73.00%\n",
      "Epoch: 324/500 | Batch 000/024 | Cost: 1.1507\n",
      "Epoch: 324/500 training accuracy: 78.75%\n",
      "Epoch: 325/500 | Batch 000/024 | Cost: 0.7121\n",
      "Epoch: 325/500 training accuracy: 73.20%\n",
      "Epoch: 326/500 | Batch 000/024 | Cost: 0.8528\n",
      "Epoch: 326/500 training accuracy: 77.03%\n",
      "Epoch: 327/500 | Batch 000/024 | Cost: 0.9442\n",
      "Epoch: 327/500 training accuracy: 81.98%\n",
      "Epoch: 328/500 | Batch 000/024 | Cost: 0.7941\n",
      "Epoch: 328/500 training accuracy: 82.18%\n",
      "Epoch: 329/500 | Batch 000/024 | Cost: 0.6579\n",
      "Epoch: 329/500 training accuracy: 81.52%\n",
      "Epoch: 330/500 | Batch 000/024 | Cost: 0.6525\n",
      "Epoch: 330/500 training accuracy: 79.08%\n",
      "Epoch: 331/500 | Batch 000/024 | Cost: 0.7063\n",
      "Epoch: 331/500 training accuracy: 78.28%\n",
      "Epoch: 332/500 | Batch 000/024 | Cost: 0.6676\n",
      "Epoch: 332/500 training accuracy: 77.76%\n",
      "Epoch: 333/500 | Batch 000/024 | Cost: 1.0876\n",
      "Epoch: 333/500 training accuracy: 79.67%\n",
      "Epoch: 334/500 | Batch 000/024 | Cost: 0.8159\n",
      "Epoch: 334/500 training accuracy: 77.82%\n",
      "Epoch: 335/500 | Batch 000/024 | Cost: 0.9163\n",
      "Epoch: 335/500 training accuracy: 78.15%\n",
      "Epoch: 336/500 | Batch 000/024 | Cost: 0.8219\n",
      "Epoch: 336/500 training accuracy: 66.60%\n",
      "Epoch: 337/500 | Batch 000/024 | Cost: 1.6525\n",
      "Epoch: 337/500 training accuracy: 75.31%\n",
      "Epoch: 338/500 | Batch 000/024 | Cost: 1.1761\n",
      "Epoch: 338/500 training accuracy: 77.95%\n",
      "Epoch: 339/500 | Batch 000/024 | Cost: 0.7369\n",
      "Epoch: 339/500 training accuracy: 80.53%\n",
      "Epoch: 340/500 | Batch 000/024 | Cost: 0.6932\n",
      "Epoch: 340/500 training accuracy: 80.59%\n",
      "Epoch: 341/500 | Batch 000/024 | Cost: 0.8126\n",
      "Epoch: 341/500 training accuracy: 79.08%\n",
      "Epoch: 342/500 | Batch 000/024 | Cost: 0.6466\n",
      "Epoch: 342/500 training accuracy: 79.41%\n",
      "Epoch: 343/500 | Batch 000/024 | Cost: 0.7697\n",
      "Epoch: 343/500 training accuracy: 80.07%\n",
      "Epoch: 344/500 | Batch 000/024 | Cost: 0.4763\n",
      "Epoch: 344/500 training accuracy: 76.17%\n",
      "Epoch: 345/500 | Batch 000/024 | Cost: 0.6575\n",
      "Epoch: 345/500 training accuracy: 78.35%\n",
      "Epoch: 346/500 | Batch 000/024 | Cost: 0.7453\n",
      "Epoch: 346/500 training accuracy: 81.78%\n",
      "Epoch: 347/500 | Batch 000/024 | Cost: 0.7883\n",
      "Epoch: 347/500 training accuracy: 79.80%\n",
      "Epoch: 348/500 | Batch 000/024 | Cost: 0.6697\n",
      "Epoch: 348/500 training accuracy: 79.08%\n",
      "Epoch: 349/500 | Batch 000/024 | Cost: 0.6243\n",
      "Epoch: 349/500 training accuracy: 78.15%\n",
      "Epoch: 350/500 | Batch 000/024 | Cost: 0.8510\n",
      "Epoch: 350/500 training accuracy: 72.87%\n",
      "Epoch: 351/500 | Batch 000/024 | Cost: 0.9840\n",
      "Epoch: 351/500 training accuracy: 82.44%\n",
      "Epoch: 352/500 | Batch 000/024 | Cost: 0.6825\n",
      "Epoch: 352/500 training accuracy: 82.84%\n",
      "Epoch: 353/500 | Batch 000/024 | Cost: 0.4352\n",
      "Epoch: 353/500 training accuracy: 81.85%\n",
      "Epoch: 354/500 | Batch 000/024 | Cost: 0.6651\n",
      "Epoch: 354/500 training accuracy: 84.55%\n",
      "Epoch: 355/500 | Batch 000/024 | Cost: 0.5133\n",
      "Epoch: 355/500 training accuracy: 80.66%\n",
      "Epoch: 356/500 | Batch 000/024 | Cost: 0.6065\n",
      "Epoch: 356/500 training accuracy: 82.44%\n",
      "Epoch: 357/500 | Batch 000/024 | Cost: 0.6842\n",
      "Epoch: 357/500 training accuracy: 83.17%\n",
      "Epoch: 358/500 | Batch 000/024 | Cost: 0.7396\n",
      "Epoch: 358/500 training accuracy: 86.93%\n",
      "Epoch: 359/500 | Batch 000/024 | Cost: 0.6004\n",
      "Epoch: 359/500 training accuracy: 82.77%\n",
      "Epoch: 360/500 | Batch 000/024 | Cost: 0.5814\n",
      "Epoch: 360/500 training accuracy: 83.63%\n",
      "Epoch: 361/500 | Batch 000/024 | Cost: 0.3920\n",
      "Epoch: 361/500 training accuracy: 81.52%\n",
      "Epoch: 362/500 | Batch 000/024 | Cost: 0.9639\n",
      "Epoch: 362/500 training accuracy: 81.65%\n",
      "Epoch: 363/500 | Batch 000/024 | Cost: 0.6101\n",
      "Epoch: 363/500 training accuracy: 70.63%\n",
      "Epoch: 364/500 | Batch 000/024 | Cost: 1.1792\n",
      "Epoch: 364/500 training accuracy: 83.63%\n",
      "Epoch: 365/500 | Batch 000/024 | Cost: 0.6311\n",
      "Epoch: 365/500 training accuracy: 84.95%\n",
      "Epoch: 366/500 | Batch 000/024 | Cost: 0.5498\n",
      "Epoch: 366/500 training accuracy: 84.95%\n",
      "Epoch: 367/500 | Batch 000/024 | Cost: 0.5531\n",
      "Epoch: 367/500 training accuracy: 82.64%\n",
      "Epoch: 368/500 | Batch 000/024 | Cost: 0.7241\n",
      "Epoch: 368/500 training accuracy: 85.41%\n",
      "Epoch: 369/500 | Batch 000/024 | Cost: 0.3798\n",
      "Epoch: 369/500 training accuracy: 86.27%\n",
      "Epoch: 370/500 | Batch 000/024 | Cost: 0.7881\n",
      "Epoch: 370/500 training accuracy: 84.36%\n",
      "Epoch: 371/500 | Batch 000/024 | Cost: 0.4843\n",
      "Epoch: 371/500 training accuracy: 83.17%\n",
      "Epoch: 372/500 | Batch 000/024 | Cost: 0.5962\n",
      "Epoch: 372/500 training accuracy: 86.27%\n",
      "Epoch: 373/500 | Batch 000/024 | Cost: 0.4890\n",
      "Epoch: 373/500 training accuracy: 83.23%\n",
      "Epoch: 374/500 | Batch 000/024 | Cost: 0.7316\n",
      "Epoch: 374/500 training accuracy: 85.54%\n",
      "Epoch: 375/500 | Batch 000/024 | Cost: 0.4297\n",
      "Epoch: 375/500 training accuracy: 79.74%\n",
      "Epoch: 376/500 | Batch 000/024 | Cost: 0.5562\n",
      "Epoch: 376/500 training accuracy: 88.58%\n",
      "Epoch: 377/500 | Batch 000/024 | Cost: 0.3382\n",
      "Epoch: 377/500 training accuracy: 83.63%\n",
      "Epoch: 378/500 | Batch 000/024 | Cost: 0.6464\n",
      "Epoch: 378/500 training accuracy: 86.01%\n",
      "Epoch: 379/500 | Batch 000/024 | Cost: 0.5848\n",
      "Epoch: 379/500 training accuracy: 85.21%\n",
      "Epoch: 380/500 | Batch 000/024 | Cost: 0.4406\n",
      "Epoch: 380/500 training accuracy: 84.42%\n",
      "Epoch: 381/500 | Batch 000/024 | Cost: 0.8327\n",
      "Epoch: 381/500 training accuracy: 87.26%\n",
      "Epoch: 382/500 | Batch 000/024 | Cost: 0.5079\n",
      "Epoch: 382/500 training accuracy: 83.89%\n",
      "Epoch: 383/500 | Batch 000/024 | Cost: 0.5857\n",
      "Epoch: 383/500 training accuracy: 85.28%\n",
      "Epoch: 384/500 | Batch 000/024 | Cost: 0.5010\n",
      "Epoch: 384/500 training accuracy: 85.68%\n",
      "Epoch: 385/500 | Batch 000/024 | Cost: 0.4964\n",
      "Epoch: 385/500 training accuracy: 83.96%\n",
      "Epoch: 386/500 | Batch 000/024 | Cost: 0.5712\n",
      "Epoch: 386/500 training accuracy: 84.03%\n",
      "Epoch: 387/500 | Batch 000/024 | Cost: 0.4210\n",
      "Epoch: 387/500 training accuracy: 86.60%\n",
      "Epoch: 388/500 | Batch 000/024 | Cost: 0.3954\n",
      "Epoch: 388/500 training accuracy: 70.96%\n",
      "Epoch: 389/500 | Batch 000/024 | Cost: 0.9505\n",
      "Epoch: 389/500 training accuracy: 86.34%\n",
      "Epoch: 390/500 | Batch 000/024 | Cost: 0.4685\n",
      "Epoch: 390/500 training accuracy: 83.50%\n",
      "Epoch: 391/500 | Batch 000/024 | Cost: 0.5546\n",
      "Epoch: 391/500 training accuracy: 86.60%\n",
      "Epoch: 392/500 | Batch 000/024 | Cost: 0.4836\n",
      "Epoch: 392/500 training accuracy: 86.07%\n",
      "Epoch: 393/500 | Batch 000/024 | Cost: 0.6245\n",
      "Epoch: 393/500 training accuracy: 80.99%\n",
      "Epoch: 394/500 | Batch 000/024 | Cost: 0.8960\n",
      "Epoch: 394/500 training accuracy: 86.93%\n",
      "Epoch: 395/500 | Batch 000/024 | Cost: 0.5953\n",
      "Epoch: 395/500 training accuracy: 86.47%\n",
      "Epoch: 396/500 | Batch 000/024 | Cost: 0.5413\n",
      "Epoch: 396/500 training accuracy: 86.73%\n",
      "Epoch: 397/500 | Batch 000/024 | Cost: 0.4402\n",
      "Epoch: 397/500 training accuracy: 88.45%\n",
      "Epoch: 398/500 | Batch 000/024 | Cost: 0.4678\n",
      "Epoch: 398/500 training accuracy: 86.93%\n",
      "Epoch: 399/500 | Batch 000/024 | Cost: 0.6652\n",
      "Epoch: 399/500 training accuracy: 87.99%\n",
      "Epoch: 400/500 | Batch 000/024 | Cost: 0.4047\n",
      "Epoch: 400/500 training accuracy: 88.38%\n",
      "Epoch: 401/500 | Batch 000/024 | Cost: 0.5237\n",
      "Epoch: 401/500 training accuracy: 86.60%\n",
      "Epoch: 402/500 | Batch 000/024 | Cost: 0.5222\n",
      "Epoch: 402/500 training accuracy: 74.65%\n",
      "Epoch: 403/500 | Batch 000/024 | Cost: 0.8239\n",
      "Epoch: 403/500 training accuracy: 86.34%\n",
      "Epoch: 404/500 | Batch 000/024 | Cost: 0.5031\n",
      "Epoch: 404/500 training accuracy: 89.83%\n",
      "Epoch: 405/500 | Batch 000/024 | Cost: 0.5503\n",
      "Epoch: 405/500 training accuracy: 87.85%\n",
      "Epoch: 406/500 | Batch 000/024 | Cost: 0.2034\n",
      "Epoch: 406/500 training accuracy: 90.56%\n",
      "Epoch: 407/500 | Batch 000/024 | Cost: 0.3972\n",
      "Epoch: 407/500 training accuracy: 73.53%\n",
      "Epoch: 408/500 | Batch 000/024 | Cost: 0.8232\n",
      "Epoch: 408/500 training accuracy: 89.17%\n",
      "Epoch: 409/500 | Batch 000/024 | Cost: 0.4694\n",
      "Epoch: 409/500 training accuracy: 88.78%\n",
      "Epoch: 410/500 | Batch 000/024 | Cost: 0.3543\n",
      "Epoch: 410/500 training accuracy: 88.91%\n",
      "Epoch: 411/500 | Batch 000/024 | Cost: 0.2874\n",
      "Epoch: 411/500 training accuracy: 88.51%\n",
      "Epoch: 412/500 | Batch 000/024 | Cost: 0.2833\n",
      "Epoch: 412/500 training accuracy: 87.92%\n",
      "Epoch: 413/500 | Batch 000/024 | Cost: 0.3764\n",
      "Epoch: 413/500 training accuracy: 87.52%\n",
      "Epoch: 414/500 | Batch 000/024 | Cost: 0.2641\n",
      "Epoch: 414/500 training accuracy: 87.99%\n",
      "Epoch: 415/500 | Batch 000/024 | Cost: 0.4866\n",
      "Epoch: 415/500 training accuracy: 91.49%\n",
      "Epoch: 416/500 | Batch 000/024 | Cost: 0.4779\n",
      "Epoch: 416/500 training accuracy: 87.99%\n",
      "Epoch: 417/500 | Batch 000/024 | Cost: 0.3344\n",
      "Epoch: 417/500 training accuracy: 82.05%\n",
      "Epoch: 418/500 | Batch 000/024 | Cost: 0.6311\n",
      "Epoch: 418/500 training accuracy: 87.59%\n",
      "Epoch: 419/500 | Batch 000/024 | Cost: 0.4296\n",
      "Epoch: 419/500 training accuracy: 88.58%\n",
      "Epoch: 420/500 | Batch 000/024 | Cost: 0.4719\n",
      "Epoch: 420/500 training accuracy: 85.87%\n",
      "Epoch: 421/500 | Batch 000/024 | Cost: 0.5810\n",
      "Epoch: 421/500 training accuracy: 90.30%\n",
      "Epoch: 422/500 | Batch 000/024 | Cost: 0.3413\n",
      "Epoch: 422/500 training accuracy: 89.24%\n",
      "Epoch: 423/500 | Batch 000/024 | Cost: 0.4765\n",
      "Epoch: 423/500 training accuracy: 89.64%\n",
      "Epoch: 424/500 | Batch 000/024 | Cost: 0.4150\n",
      "Epoch: 424/500 training accuracy: 85.35%\n",
      "Epoch: 425/500 | Batch 000/024 | Cost: 0.4288\n",
      "Epoch: 425/500 training accuracy: 87.92%\n",
      "Epoch: 426/500 | Batch 000/024 | Cost: 0.4000\n",
      "Epoch: 426/500 training accuracy: 88.12%\n",
      "Epoch: 427/500 | Batch 000/024 | Cost: 0.5580\n",
      "Epoch: 427/500 training accuracy: 89.44%\n",
      "Epoch: 428/500 | Batch 000/024 | Cost: 0.2162\n",
      "Epoch: 428/500 training accuracy: 87.66%\n",
      "Epoch: 429/500 | Batch 000/024 | Cost: 0.3590\n",
      "Epoch: 429/500 training accuracy: 86.53%\n",
      "Epoch: 430/500 | Batch 000/024 | Cost: 0.5072\n",
      "Epoch: 430/500 training accuracy: 90.23%\n",
      "Epoch: 431/500 | Batch 000/024 | Cost: 0.2639\n",
      "Epoch: 431/500 training accuracy: 81.19%\n",
      "Epoch: 432/500 | Batch 000/024 | Cost: 0.5327\n",
      "Epoch: 432/500 training accuracy: 89.31%\n",
      "Epoch: 433/500 | Batch 000/024 | Cost: 0.4628\n",
      "Epoch: 433/500 training accuracy: 86.73%\n",
      "Epoch: 434/500 | Batch 000/024 | Cost: 0.4075\n",
      "Epoch: 434/500 training accuracy: 90.63%\n",
      "Epoch: 435/500 | Batch 000/024 | Cost: 0.4306\n",
      "Epoch: 435/500 training accuracy: 87.26%\n",
      "Epoch: 436/500 | Batch 000/024 | Cost: 0.5878\n",
      "Epoch: 436/500 training accuracy: 86.47%\n",
      "Epoch: 437/500 | Batch 000/024 | Cost: 0.4709\n",
      "Epoch: 437/500 training accuracy: 91.22%\n",
      "Epoch: 438/500 | Batch 000/024 | Cost: 0.4826\n",
      "Epoch: 438/500 training accuracy: 86.60%\n",
      "Epoch: 439/500 | Batch 000/024 | Cost: 0.4693\n",
      "Epoch: 439/500 training accuracy: 87.72%\n",
      "Epoch: 440/500 | Batch 000/024 | Cost: 0.5401\n",
      "Epoch: 440/500 training accuracy: 89.50%\n",
      "Epoch: 441/500 | Batch 000/024 | Cost: 0.5653\n",
      "Epoch: 441/500 training accuracy: 89.44%\n",
      "Epoch: 442/500 | Batch 000/024 | Cost: 0.3082\n",
      "Epoch: 442/500 training accuracy: 90.36%\n",
      "Epoch: 443/500 | Batch 000/024 | Cost: 0.2204\n",
      "Epoch: 443/500 training accuracy: 89.04%\n",
      "Epoch: 444/500 | Batch 000/024 | Cost: 0.4023\n",
      "Epoch: 444/500 training accuracy: 93.07%\n",
      "Epoch: 445/500 | Batch 000/024 | Cost: 0.1962\n",
      "Epoch: 445/500 training accuracy: 91.42%\n",
      "Epoch: 446/500 | Batch 000/024 | Cost: 0.4603\n",
      "Epoch: 446/500 training accuracy: 88.65%\n",
      "Epoch: 447/500 | Batch 000/024 | Cost: 0.2695\n",
      "Epoch: 447/500 training accuracy: 90.10%\n",
      "Epoch: 448/500 | Batch 000/024 | Cost: 0.3054\n",
      "Epoch: 448/500 training accuracy: 93.07%\n",
      "Epoch: 449/500 | Batch 000/024 | Cost: 0.2629\n",
      "Epoch: 449/500 training accuracy: 89.97%\n",
      "Epoch: 450/500 | Batch 000/024 | Cost: 0.4523\n",
      "Epoch: 450/500 training accuracy: 91.49%\n",
      "Epoch: 451/500 | Batch 000/024 | Cost: 0.2303\n",
      "Epoch: 451/500 training accuracy: 88.84%\n",
      "Epoch: 452/500 | Batch 000/024 | Cost: 0.2710\n",
      "Epoch: 452/500 training accuracy: 88.51%\n",
      "Epoch: 453/500 | Batch 000/024 | Cost: 0.5650\n",
      "Epoch: 453/500 training accuracy: 92.21%\n",
      "Epoch: 454/500 | Batch 000/024 | Cost: 0.3111\n",
      "Epoch: 454/500 training accuracy: 90.03%\n",
      "Epoch: 455/500 | Batch 000/024 | Cost: 0.3869\n",
      "Epoch: 455/500 training accuracy: 90.36%\n",
      "Epoch: 456/500 | Batch 000/024 | Cost: 0.2009\n",
      "Epoch: 456/500 training accuracy: 92.28%\n",
      "Epoch: 457/500 | Batch 000/024 | Cost: 0.3609\n",
      "Epoch: 457/500 training accuracy: 91.75%\n",
      "Epoch: 458/500 | Batch 000/024 | Cost: 0.2949\n",
      "Epoch: 458/500 training accuracy: 92.21%\n",
      "Epoch: 459/500 | Batch 000/024 | Cost: 0.2708\n",
      "Epoch: 459/500 training accuracy: 91.62%\n",
      "Epoch: 460/500 | Batch 000/024 | Cost: 0.3794\n",
      "Epoch: 460/500 training accuracy: 90.83%\n",
      "Epoch: 461/500 | Batch 000/024 | Cost: 0.3071\n",
      "Epoch: 461/500 training accuracy: 87.66%\n",
      "Epoch: 462/500 | Batch 000/024 | Cost: 0.5471\n",
      "Epoch: 462/500 training accuracy: 90.43%\n",
      "Epoch: 463/500 | Batch 000/024 | Cost: 0.3241\n",
      "Epoch: 463/500 training accuracy: 91.42%\n",
      "Epoch: 464/500 | Batch 000/024 | Cost: 0.2971\n",
      "Epoch: 464/500 training accuracy: 87.26%\n",
      "Epoch: 465/500 | Batch 000/024 | Cost: 0.4455\n",
      "Epoch: 465/500 training accuracy: 88.25%\n",
      "Epoch: 466/500 | Batch 000/024 | Cost: 0.5506\n",
      "Epoch: 466/500 training accuracy: 81.19%\n",
      "Epoch: 467/500 | Batch 000/024 | Cost: 1.2809\n",
      "Epoch: 467/500 training accuracy: 93.14%\n",
      "Epoch: 468/500 | Batch 000/024 | Cost: 0.2922\n",
      "Epoch: 468/500 training accuracy: 85.68%\n",
      "Epoch: 469/500 | Batch 000/024 | Cost: 0.4675\n",
      "Epoch: 469/500 training accuracy: 93.20%\n",
      "Epoch: 470/500 | Batch 000/024 | Cost: 0.3785\n",
      "Epoch: 470/500 training accuracy: 94.19%\n",
      "Epoch: 471/500 | Batch 000/024 | Cost: 0.2795\n",
      "Epoch: 471/500 training accuracy: 93.14%\n",
      "Epoch: 472/500 | Batch 000/024 | Cost: 0.2085\n",
      "Epoch: 472/500 training accuracy: 88.18%\n",
      "Epoch: 473/500 | Batch 000/024 | Cost: 0.2672\n",
      "Epoch: 473/500 training accuracy: 87.99%\n",
      "Epoch: 474/500 | Batch 000/024 | Cost: 0.4295\n",
      "Epoch: 474/500 training accuracy: 90.30%\n",
      "Epoch: 475/500 | Batch 000/024 | Cost: 0.4321\n",
      "Epoch: 475/500 training accuracy: 91.62%\n",
      "Epoch: 476/500 | Batch 000/024 | Cost: 0.2387\n",
      "Epoch: 476/500 training accuracy: 92.15%\n",
      "Epoch: 477/500 | Batch 000/024 | Cost: 0.2944\n",
      "Epoch: 477/500 training accuracy: 86.27%\n",
      "Epoch: 478/500 | Batch 000/024 | Cost: 0.4520\n",
      "Epoch: 478/500 training accuracy: 89.31%\n",
      "Epoch: 479/500 | Batch 000/024 | Cost: 0.5681\n",
      "Epoch: 479/500 training accuracy: 93.66%\n",
      "Epoch: 480/500 | Batch 000/024 | Cost: 0.3943\n",
      "Epoch: 480/500 training accuracy: 92.67%\n",
      "Epoch: 481/500 | Batch 000/024 | Cost: 0.3133\n",
      "Epoch: 481/500 training accuracy: 90.83%\n",
      "Epoch: 482/500 | Batch 000/024 | Cost: 0.1730\n",
      "Epoch: 482/500 training accuracy: 92.48%\n",
      "Epoch: 483/500 | Batch 000/024 | Cost: 0.2709\n",
      "Epoch: 483/500 training accuracy: 88.84%\n",
      "Epoch: 484/500 | Batch 000/024 | Cost: 0.4562\n",
      "Epoch: 484/500 training accuracy: 94.06%\n",
      "Epoch: 485/500 | Batch 000/024 | Cost: 0.2442\n",
      "Epoch: 485/500 training accuracy: 92.01%\n",
      "Epoch: 486/500 | Batch 000/024 | Cost: 0.3778\n",
      "Epoch: 486/500 training accuracy: 91.22%\n",
      "Epoch: 487/500 | Batch 000/024 | Cost: 0.3557\n",
      "Epoch: 487/500 training accuracy: 93.27%\n",
      "Epoch: 488/500 | Batch 000/024 | Cost: 0.2123\n",
      "Epoch: 488/500 training accuracy: 90.89%\n",
      "Epoch: 489/500 | Batch 000/024 | Cost: 0.4565\n",
      "Epoch: 489/500 training accuracy: 86.60%\n",
      "Epoch: 490/500 | Batch 000/024 | Cost: 0.5074\n",
      "Epoch: 490/500 training accuracy: 90.96%\n",
      "Epoch: 491/500 | Batch 000/024 | Cost: 0.3061\n",
      "Epoch: 491/500 training accuracy: 92.94%\n",
      "Epoch: 492/500 | Batch 000/024 | Cost: 0.3829\n",
      "Epoch: 492/500 training accuracy: 89.24%\n",
      "Epoch: 493/500 | Batch 000/024 | Cost: 0.3341\n",
      "Epoch: 493/500 training accuracy: 91.95%\n",
      "Epoch: 494/500 | Batch 000/024 | Cost: 0.2670\n",
      "Epoch: 494/500 training accuracy: 93.80%\n",
      "Epoch: 495/500 | Batch 000/024 | Cost: 0.2657\n",
      "Epoch: 495/500 training accuracy: 91.02%\n",
      "Epoch: 496/500 | Batch 000/024 | Cost: 0.4679\n",
      "Epoch: 496/500 training accuracy: 92.34%\n",
      "Epoch: 497/500 | Batch 000/024 | Cost: 0.1377\n",
      "Epoch: 497/500 training accuracy: 95.05%\n",
      "Epoch: 498/500 | Batch 000/024 | Cost: 0.1718\n",
      "Epoch: 498/500 training accuracy: 92.67%\n",
      "Epoch: 499/500 | Batch 000/024 | Cost: 0.2039\n",
      "Epoch: 499/500 training accuracy: 93.86%\n",
      "Epoch: 500/500 | Batch 000/024 | Cost: 0.3324\n",
      "Epoch: 500/500 training accuracy: 94.98%\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, data_loader):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for features, targets in data_loader:\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100\n",
    "\n",
    "num_epochs=500       \n",
    "for epoch in range(num_epochs):\n",
    "    model = model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 50:\n",
    "            print ('Epoch: %03d/%03d | Batch %03d/%03d | Cost: %.4f' \n",
    "                   %(epoch+1, num_epochs, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "    \n",
    "    model = model.eval()\n",
    "    print('Epoch: %03d/%03d training accuracy: %.2f%%' % (\n",
    "          epoch+1, num_epochs, \n",
    "          compute_accuracy(model, train_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b682f0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 33.90%\n"
     ]
    }
   ],
   "source": [
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    print('Test accuracy: %.2f%%' % (compute_accuracy(model, val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "83c8b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, train_loader,learning_rate, n_epochs, loss_func, optimizer, device):\n",
    "    train_loss = []\n",
    "    train_acc=[]\n",
    "    val_loss=[]\n",
    "    val_acc=[]\n",
    "    # training the model\n",
    "    for epoch in range(n_epochs):\n",
    "        tr_loss = 0\n",
    "        tr_correct = 0\n",
    "        v_loss=0\n",
    "        v_correct=0\n",
    "        # getting the training set\n",
    "    \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # clearing the Gradients of the model parameters\n",
    "            optimizer.zero_grad()\n",
    "            # prediction for training\n",
    "            net_out = model(data)\n",
    "            # computing the training and validation loss\n",
    "            loss_train = loss_func(net_out, target)\n",
    "            # computing the updated weights of all the model parameters\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "            tr_loss += loss_train.item()\n",
    "            pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "            tr_correct += pred.eq(target.data).sum()\n",
    "            \n",
    "        train_loss.append(tr_loss/len(train_loader.dataset))\n",
    "        train_acc.append(tr_correct/len(train_loader.dataset))\n",
    "    \n",
    "        print(\"------------------------------------------------------------------------------------------------\")\n",
    "        print('Epoch: ', epoch)\n",
    "        print('Training Loss:', tr_loss / len(train_loader.dataset)) \n",
    "        print(\"Training Accuracy:\", 100. * tr_correct.item() / len(train_loader.dataset))\n",
    "        print(\"------------------------------------------------------------------------------------------------\") \n",
    "        \n",
    "        \n",
    "    return net, train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "fbcc2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_network(path , model):\n",
    "  torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7be970cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network(path, model):\n",
    "  model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "d142ed44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vitualization(train_loss,train_accuracy):\n",
    "\n",
    "  plt.figure(figsize=(3,3))\n",
    "  plt.title(\"Plot the resulting loss over time\") \n",
    "  plt.plot(train_loss, label='Training Loss')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "  plt.figure(figsize=(3,3))\n",
    "  plt.title(\"Plot the Resulting Acuracy over time\")\n",
    "  plt.plot(train_accuracy, label='Training Accuracy')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8a1ce718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv_1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool_1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv_2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool_2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (linear_1): Linear(in_features=4096, out_features=100, bias=True)\n",
      ")\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  0\n",
      "Training Loss: 0.07248493074977358\n",
      "Training Accuracy: 1.7821782178217822\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  1\n",
      "Training Loss: 0.07084711219611341\n",
      "Training Accuracy: 3.036303630363036\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  2\n",
      "Training Loss: 0.06941512803433358\n",
      "Training Accuracy: 3.4983498349834985\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  3\n",
      "Training Loss: 0.06725382065222209\n",
      "Training Accuracy: 5.6105610561056105\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  4\n",
      "Training Loss: 0.06515989901602465\n",
      "Training Accuracy: 8.382838283828383\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  5\n",
      "Training Loss: 0.06294648765337349\n",
      "Training Accuracy: 10.495049504950495\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  6\n",
      "Training Loss: 0.0617316991975992\n",
      "Training Accuracy: 10.825082508250825\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  7\n",
      "Training Loss: 0.05979071890953744\n",
      "Training Accuracy: 12.211221122112212\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  8\n",
      "Training Loss: 0.05809742058857833\n",
      "Training Accuracy: 13.795379537953796\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  9\n",
      "Training Loss: 0.05683379126067209\n",
      "Training Accuracy: 16.237623762376238\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  10\n",
      "Training Loss: 0.05527110949601277\n",
      "Training Accuracy: 18.151815181518153\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  11\n",
      "Training Loss: 0.05402125840139861\n",
      "Training Accuracy: 18.217821782178216\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  12\n",
      "Training Loss: 0.05261046028766695\n",
      "Training Accuracy: 21.782178217821784\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  13\n",
      "Training Loss: 0.051669134794682166\n",
      "Training Accuracy: 21.122112211221122\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  14\n",
      "Training Loss: 0.050327240122426856\n",
      "Training Accuracy: 22.77227722772277\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  15\n",
      "Training Loss: 0.048909610018084936\n",
      "Training Accuracy: 24.356435643564357\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  16\n",
      "Training Loss: 0.047309807660949506\n",
      "Training Accuracy: 27.458745874587457\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  17\n",
      "Training Loss: 0.04613911216408506\n",
      "Training Accuracy: 27.986798679867988\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  18\n",
      "Training Loss: 0.04475659851980682\n",
      "Training Accuracy: 30.957095709570957\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  19\n",
      "Training Loss: 0.04407779076705278\n",
      "Training Accuracy: 31.221122112211223\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  20\n",
      "Training Loss: 0.041868141854163446\n",
      "Training Accuracy: 32.93729372937294\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  21\n",
      "Training Loss: 0.04074238330224166\n",
      "Training Accuracy: 34.71947194719472\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  22\n",
      "Training Loss: 0.04014730217433212\n",
      "Training Accuracy: 35.51155115511551\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  23\n",
      "Training Loss: 0.03940057896151401\n",
      "Training Accuracy: 37.161716171617165\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  24\n",
      "Training Loss: 0.03748058194767917\n",
      "Training Accuracy: 38.28382838283829\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  25\n",
      "Training Loss: 0.036304501102309025\n",
      "Training Accuracy: 42.04620462046205\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  26\n",
      "Training Loss: 0.035858333543582324\n",
      "Training Accuracy: 41.056105610561055\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  27\n",
      "Training Loss: 0.034206299813273715\n",
      "Training Accuracy: 43.4983498349835\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  28\n",
      "Training Loss: 0.03347053197350833\n",
      "Training Accuracy: 46.20462046204621\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  29\n",
      "Training Loss: 0.031232454438414117\n",
      "Training Accuracy: 48.118811881188115\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  30\n",
      "Training Loss: 0.03172016261827828\n",
      "Training Accuracy: 48.77887788778878\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  31\n",
      "Training Loss: 0.030201902483949567\n",
      "Training Accuracy: 50.62706270627063\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  32\n",
      "Training Loss: 0.028988649978889492\n",
      "Training Accuracy: 51.881188118811885\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  33\n",
      "Training Loss: 0.027792673535866314\n",
      "Training Accuracy: 52.01320132013201\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  34\n",
      "Training Loss: 0.028252836029128273\n",
      "Training Accuracy: 53.72937293729373\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  35\n",
      "Training Loss: 0.025830124707111823\n",
      "Training Accuracy: 56.96369636963696\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  36\n",
      "Training Loss: 0.02468630826906009\n",
      "Training Accuracy: 59.07590759075907\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  37\n",
      "Training Loss: 0.025336888993140495\n",
      "Training Accuracy: 56.6996699669967\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  38\n",
      "Training Loss: 0.0239882225250647\n",
      "Training Accuracy: 59.66996699669967\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  39\n",
      "Training Loss: 0.023483075796574257\n",
      "Training Accuracy: 59.40594059405941\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  40\n",
      "Training Loss: 0.02306523134212683\n",
      "Training Accuracy: 61.386138613861384\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  41\n",
      "Training Loss: 0.02098784608022608\n",
      "Training Accuracy: 64.62046204620462\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  42\n",
      "Training Loss: 0.021661866616101155\n",
      "Training Accuracy: 63.366336633663366\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  43\n",
      "Training Loss: 0.020646990053724536\n",
      "Training Accuracy: 63.76237623762376\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  44\n",
      "Training Loss: 0.01937042242623005\n",
      "Training Accuracy: 68.18481848184818\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  45\n",
      "Training Loss: 0.01929601065003046\n",
      "Training Accuracy: 67.92079207920793\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  46\n",
      "Training Loss: 0.01866584815601311\n",
      "Training Accuracy: 67.65676567656766\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  47\n",
      "Training Loss: 0.01947932207938468\n",
      "Training Accuracy: 68.51485148514851\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  48\n",
      "Training Loss: 0.017175812335691043\n",
      "Training Accuracy: 70.42904290429043\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  49\n",
      "Training Loss: 0.017883957101173527\n",
      "Training Accuracy: 68.7128712871287\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  50\n",
      "Training Loss: 0.016804872290922864\n",
      "Training Accuracy: 69.76897689768977\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  51\n",
      "Training Loss: 0.0155976936368659\n",
      "Training Accuracy: 73.72937293729373\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  52\n",
      "Training Loss: 0.015611287705575672\n",
      "Training Accuracy: 72.8052805280528\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  53\n",
      "Training Loss: 0.01492731779715409\n",
      "Training Accuracy: 75.04950495049505\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  54\n",
      "Training Loss: 0.015417013152597761\n",
      "Training Accuracy: 73.53135313531354\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  55\n",
      "Training Loss: 0.0144556197789636\n",
      "Training Accuracy: 75.18151815181518\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  56\n",
      "Training Loss: 0.014145880839218795\n",
      "Training Accuracy: 75.77557755775578\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  57\n",
      "Training Loss: 0.01402478017429314\n",
      "Training Accuracy: 75.9075907590759\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  58\n",
      "Training Loss: 0.014282077452530562\n",
      "Training Accuracy: 74.58745874587459\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  59\n",
      "Training Loss: 0.01207908269989215\n",
      "Training Accuracy: 78.87788778877888\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  60\n",
      "Training Loss: 0.013604707647078109\n",
      "Training Accuracy: 76.56765676567657\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  61\n",
      "Training Loss: 0.012707982224599757\n",
      "Training Accuracy: 77.02970297029702\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  62\n",
      "Training Loss: 0.013003650376505584\n",
      "Training Accuracy: 77.62376237623762\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  63\n",
      "Training Loss: 0.011885288013483432\n",
      "Training Accuracy: 80.52805280528052\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  64\n",
      "Training Loss: 0.01102574109637698\n",
      "Training Accuracy: 80.26402640264027\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  65\n",
      "Training Loss: 0.011226093749401986\n",
      "Training Accuracy: 80.06600660066006\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  66\n",
      "Training Loss: 0.011544805647122978\n",
      "Training Accuracy: 79.8019801980198\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  67\n",
      "Training Loss: 0.01034243887800588\n",
      "Training Accuracy: 81.25412541254126\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  68\n",
      "Training Loss: 0.009728149907423718\n",
      "Training Accuracy: 83.63036303630363\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  69\n",
      "Training Loss: 0.009755295101958926\n",
      "Training Accuracy: 84.42244224422443\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  70\n",
      "Training Loss: 0.00932135267226216\n",
      "Training Accuracy: 83.4983498349835\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  71\n",
      "Training Loss: 0.009095055966487419\n",
      "Training Accuracy: 83.63036303630363\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  72\n",
      "Training Loss: 0.008080397285643977\n",
      "Training Accuracy: 85.61056105610561\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  73\n",
      "Training Loss: 0.00814057999711619\n",
      "Training Accuracy: 85.54455445544555\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  74\n",
      "Training Loss: 0.00943337324822303\n",
      "Training Accuracy: 84.0924092409241\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  75\n",
      "Training Loss: 0.008862471580505371\n",
      "Training Accuracy: 85.14851485148515\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  76\n",
      "Training Loss: 0.008025806699648943\n",
      "Training Accuracy: 85.94059405940594\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  77\n",
      "Training Loss: 0.007997590028020021\n",
      "Training Accuracy: 85.8085808580858\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  78\n",
      "Training Loss: 0.006826768023739553\n",
      "Training Accuracy: 87.59075907590758\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  79\n",
      "Training Loss: 0.007364332813634337\n",
      "Training Accuracy: 86.86468646864687\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  80\n",
      "Training Loss: 0.00634811010101054\n",
      "Training Accuracy: 89.5049504950495\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  81\n",
      "Training Loss: 0.007304209696971151\n",
      "Training Accuracy: 86.66666666666667\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  82\n",
      "Training Loss: 0.008172766928232149\n",
      "Training Accuracy: 85.14851485148515\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  83\n",
      "Training Loss: 0.008627473580168419\n",
      "Training Accuracy: 85.54455445544555\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  84\n",
      "Training Loss: 0.0068973458442750935\n",
      "Training Accuracy: 87.12871287128714\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  85\n",
      "Training Loss: 0.006843504899799234\n",
      "Training Accuracy: 87.06270627062706\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  86\n",
      "Training Loss: 0.006936174168838526\n",
      "Training Accuracy: 87.39273927392739\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  87\n",
      "Training Loss: 0.0064211200762896645\n",
      "Training Accuracy: 89.9009900990099\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  88\n",
      "Training Loss: 0.006444207335462665\n",
      "Training Accuracy: 88.05280528052805\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  89\n",
      "Training Loss: 0.005684925822338255\n",
      "Training Accuracy: 90.23102310231023\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  90\n",
      "Training Loss: 0.006042447656688123\n",
      "Training Accuracy: 89.24092409240924\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  91\n",
      "Training Loss: 0.005474685855431132\n",
      "Training Accuracy: 90.16501650165017\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  92\n",
      "Training Loss: 0.005425231507902492\n",
      "Training Accuracy: 91.08910891089108\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  93\n",
      "Training Loss: 0.006090252789923854\n",
      "Training Accuracy: 89.57095709570957\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  94\n",
      "Training Loss: 0.005689315237227839\n",
      "Training Accuracy: 90.0990099009901\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  95\n",
      "Training Loss: 0.005878267006905559\n",
      "Training Accuracy: 90.6930693069307\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  96\n",
      "Training Loss: 0.005717428594139149\n",
      "Training Accuracy: 89.96699669966996\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  97\n",
      "Training Loss: 0.005543276815131159\n",
      "Training Accuracy: 89.96699669966996\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  98\n",
      "Training Loss: 0.004548746524470867\n",
      "Training Accuracy: 93.06930693069307\n",
      "------------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------------\n",
      "Epoch:  99\n",
      "Training Loss: 0.004362449551572894\n",
      "Training Accuracy: 92.40924092409242\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lr=0.005\n",
    "n_epoch=100\n",
    "# defining the model\n",
    "# this statement tell our code if there gpu available on our machine or not.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = ConvNet(num_classes=100)\n",
    "model = model.to(device)\n",
    "# defining the optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr, momentum=0.9)\n",
    "# defining the loss function\n",
    "loss_func = nn.NLLLoss()\n",
    "#loss_func = CrossEntropyLoss()\n",
    "    \n",
    "print(model)\n",
    "\n",
    "model, train_loss, train_accuracy = train(model, train_loader,lr, n_epoch ,loss_func, optimizer, device)\n",
    "save_network(\"Model.pt\", model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "47d1cc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_vitualization(train_loss, train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "272bd5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    test_acc=[]\n",
    "    test_loss=[]\n",
    "    t_loss =0\n",
    "    correct=0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            net_out = model(data)\n",
    "            loss = loss_func(net_out, target)\n",
    "            \n",
    "            t_loss += loss.item()\n",
    "            pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data).sum()\n",
    "            \n",
    "        test_loss.append(t_loss/len(test_loader.dataset))\n",
    "        test_acc.append(correct/len(test_loader.dataset))\n",
    "        \n",
    "        print(\"------------------------------------------------------------------------------------------------\")\n",
    "        print('Testing Loss:', t_loss/len(test_loader.dataset)) \n",
    "        print(\"Testing Accuracy:\", 100. * correct.item() / len(test_loader.dataset))\n",
    "        print(\"------------------------------------------------------------------------------------------------\") \n",
    "        \n",
    "        \n",
    "    \n",
    "    return  test_accuracy, test_loss\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "8dee808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf(actuals, predictions):\n",
    "  print('F1 score: %f' % f1_score(actuals, predictions, average='micro'))\n",
    "  print('Accuracy score: %f' % accuracy_score(actuals, predictions))\n",
    "\n",
    "  print('------------------------Confusion matrix----------------------------------')\n",
    "  import seaborn as sns\n",
    "  plt.figure(figsize=(10,9))\n",
    "  ax = sns.heatmap(confusion_matrix(actuals, predictions), annot=True, vmax=30)\n",
    "  ax.set_xlabel('Predicted')\n",
    "  ax.set_ylabel('True')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "2a266037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "Testing Loss: 0.08616129596058915\n",
      "Testing Accuracy: 33.170731707317074\n",
      "------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "load_network(\"Model.pt\",model)\n",
    "test_accuracy,test_loss = test(model, device, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d65623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa2f251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdd223f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
